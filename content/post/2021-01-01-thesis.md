---
title: About me
date: 2021-01-01
---

# Introduction {#sec:intro}

Food systems have a substantial global environmental footprint by
contributing to problems such as climate change, biodiversity loss, and
land system change [@Willett2019]. In fact, the food supply chain
accounts for one quarter of global anthropogenic Green House Gas (GHG)
emission, one third of global terrestrial acidification and about 80% of
eutrophication. Furthermore, at least 6 out of 17 of the United Nations'
Sustainable Development Goals are concerned with food
[@Poore2018a; @Johnston2016].

The food supply chain can be segmented into seven stages: 1) Land use
change, 2) Farming, 3) Animal food production, 4) Processing, 5)
Transport, 6) Retail and 7) Packaging [@Poore2018a]. GHG emission from
land use change and farming account for more than 80% of the footprint
for most foods [@Poore2018a]. The amount and kind of food that a person
habitually consumes is called a diet. Food products in a diet largely
determines its GHG footprint [@Sandstrom2018]; dairy, meat and eggs
account for 80% and plant based food for 20%. Although, the country of
origin of the food products also influences dietary emissions, mainly
because of differences in land use change [@Sandstrom2018].

Food products in overall diets have been studied in various ways such as
recipe analysis, food diaries, direct observations of food consumption
or surveys. The benefits of online recipe analysis over other methods
are: more reactive than food diaries, more scalable than direct
observations of food consumption, and less biased than surveys
[@FCRN2015; @Mouritsen2017]. An example of online recipe based work is
an analysis of the change towards meat-free diets in Germany
[@Asano2019].

A number of complications have been identified in computational
assessment of recipes for sustainability. First of all, Natural Language
Processing (NLP) is technologically complex once recipes do not come
from the same source [@VanErp2021]. Examples of NLP-issues are:
Part-Of-Speech tagging is not reliable for sentences in the food recipe
domain due to their imperative nature [@Goncalo2018; @Chang2018],
interpreting sentences in the food recipe task which are either poorly
structured by the author or contain a misleading expression
[@Goncalo2018], quantities can be expressed by numerals, expressed by
fractions or spelled out and units can be expressed as metric, imperial,
or other measurements such as teacups [@VanErp2018]. Another
complication addressed in [@VanErp2021] is that there are linguistic,
conceptual and terminological gaps between recipe and GHG emission
Knowledge Bases (KBs), that is, KBs that allocate a quantity of GHG
emission to a food product [@VanErp2021]. Issues in bridging the gap
between recipes and GHG emission KBs are: GHG emission KBs can be
structured according to a variety of food data systems developed by
different countries in different languages (such as FoodEx2, LanguaL,
GloboDiet and NEVO) [@Brown2017; @Ireland2000], and computational food
matching according to the definitions of a food data system is
challenging when food is described differently than in the food data
system [@KorousicSeljak2018]. To the best of our knowledge, no research
has been published on how to resolve the complications mentioned in this
introduction for online recipes written in the Dutch language. Therefore
our main contribution to existing literature is that we focus our
efforts on Dutch recipes only. As a result, our research question is:
*To what extent can recipe footprint assessment be automated by linking
ingredients, quantities and units from Dutch recipe text to a GHG
emission KB?* The main question is broken down into four sub-questions:

1.  To what extent can descriptions of ingredients, quantities and units
    be recognized in Dutch recipe text?

2.  To what extent can recognized descriptions of ingredients be linked
    to a GHG emission KB?

3.  To what extent can recognized descriptions of quantities and units
    be linked to a GHG emission KB?

4.  To what extent can a recipe's footprint be assessed based on a
    linked GHG emission KB?

The remainder of this paper is structured as follows. Related work is
described in Section [\[sec:rel\]](#sec:rel){reference-type="ref"
reference="sec:rel"}. Our methodology is described in Section
[\[sec:meth\]](#sec:meth){reference-type="ref" reference="sec:meth"}.
The results are shown in Section
[\[sec:res\]](#sec:res){reference-type="ref" reference="sec:res"}. In
Section [\[sec:eva\]](#sec:eva){reference-type="ref"
reference="sec:eva"}, we discuss the limitations and implications of our
results. The conclusions and future work are described in Section
[\[sec:conc\]](#sec:conc){reference-type="ref" reference="sec:conc"}.

# Related Work {#sec:rel}

We describe related work on recipe data, description recognition,
sustainable food data and ingredient linking in the following
subsections.

## Recipe data {#subsec:RNLdata}

Recipe data is available from various sources and in different formats
and languages [@VanErp2021]. We clustered publications of recipe data
along the two dimensions most relevant to this study: 1) Language
(Non-Dutch vs Dutch) and 2) ingredient annotation (with vs without a
golden standard).

The first cluster consists of Non-Dutch data sets without golden
standard. The Cooking Recipes Without Border data set (CRWB)
[@Frederic2018; @Frederic2018a] is an initiative to collaborate on a
linked open data collection of cooking recipes that follows the FAIR
model [@Wilkinson2016]. CRWB contains observations of ingredients in
dishes by users which can be used as annotation labels, but strictly
speaking are not ground truth labels. In addition, CRWB has cooking
actions described in French. The Recipe1M+ dataset consists of 1M
recipes with images of corresponding dishes scraped from non-Dutch
cooking websites [@Marin2021]. The Recipe1M+ data set contains
ingredients, units and quantities annotations which were are
automatically tagged by a 'quantity-unit-ingredient' sequence pattern.

Secondly, the cluster of related work that consists of Non-Dutch data
sets with a golden standard. RecipeDB is a structure compilation of over
100k recipes in which ingredients are labelled and linked to nutritional
values [@Batra2020]. The New York Times published a dataset that
contains 180K ingredient lines in English language in which are manually
annotated for ingredients, units and amounts [@Greene2015]. FoodBase
[@Popovski2019] is data set with recipes in English language extracted
from Allrecipes [^1] which are annotated with a rule-based food Named
Entity Recognition (NER) method. For a subset of 1,000 recipes, the
annotation was manually evaluated to create a golden standard. RecipeNLG
[@BienMichaandGilski2020] is a dataset of over 2M English language
cooking recipes for the purpose of generating recipes, it was built on
top of Recipe1M+ [@Marin2021]. A subset of 500 recipes in RecipeNLG was
manually annotated for food entities to train a NER model that was used
extract ingredients from the rest of the data set.

The third cluster consists of Dutch data sets with a golden standard. A
data set with 27K Dutch recipes from newspaper articles is published by
[@VanErp2018]. A subset of 100 recipes are manually annotated by tagging
ingredients, quantities and units in the corpus of the articles. The
final cluster consists of Dutch data sets without a golden standard. In
addition to a data set with recipes from news papers, [@VanErp2018] also
made a data set available that contains 16K recipes scraped from the
website of one of the largest Dutch supermarket chains. The recipes are
marked up with schema.org information such as ingredients, units and
quantities.

## Description recognition

Description recognition is a task that can be interpreted as sequence
tagging, that is, the tokens in a food text need to be tagged as
ingredient, unit or quantity. Sequence tagging is a structured learning
problem in which a class or label is assigned to each token in an input
sequence [@Tang2010]. We clustered previous work into lexicon-based,
rule-based and learning-based sequence tagging approaches. To start with
lexicon-based sequence tagging, [@Mazzei] compared to what extent 4
Italian computational lexicons cover the words in a recipe for
extraction of the recipe's nutritional values. A lexicon-based method to
lift ingredients is presented in [@VanErp2018], followed by Named Entity
Linking (NEL) based on string match or DBpedia spotlight [@Daiber2013].
Secondly, we discuss examples of rule-based approaches. Regular
expressions are used by [@VanErp2018] on tokens for NER of quantities
and units. In the approach of [@Haussmann2019], first parts of the
ingredient sentence that are assumed not or less relevant (e.g.
parenthesized statements) are stripped, than numerical tokens are
selected as quantities and the NLTK toolkit [^2] is used to extract the
ingredient from the remaining part of the sentence. A rule-based food
NER called FoodIE is introduced by [@Popovski2019a], it is based on the
UCREL Semantic Analysis System (USAS) tagger presented in [@Rayson2004].
Lastly we describe the learning-based approaches. Ingredient sequences
are tagged with conditional random fields in [@Greene2015]. Stanford
CoreNLP [^3] Part-of-Speech tagging and human annotation is used in
[@Chang2018] to train a model that parses ingredients and actions. A
deep learning NER tool are trained in
[@Yamakata2020; @BienMichaandGilski2020] to extract a number of recipe
named entities. A CNN-based sequence encoder based on [@Zhang2015] is
used in [@Yamaguchi2020] to detect non-ingredient such as kitchenware in
recipe ingredient lists. A Stanford NER Tagger is trained and utilized
by [@Diwan2020] to annotate the ingredients for clustering purposes.

## Sustainable food data

Sustainability data and knowledge sources for food are incoherent and
not consistently available [@VanErp2021]. Food consumption data of the
Food and Agricultural Organization (FAO) contains useful indicators of
the environmental impact of ingredients [@Quadros2019]. The SHARP
Indicators Database (SHARP-ID[^4]) is a European-wide applicable public
database for indicators of environmental sustainability of primary food
products [@Mertens2019]. A construction process and maintenance plan for
a unified food knowledge graph is published by [@Haussmann2019]. First
results of the BONSAI project, which aims for an open source dataset and
ontology for food product life cycle assessment, are presented by
[@Ghose2019]. A carbon footprint study is conducted by [@Toledo2020]
based on a dummy database since the required knowledge bases were not
available at that point of time. While sustainability data might be
available for individual ingredients, it is still rare for entire
recipes [@Speck2020; @Roos2013]. A methodology to assess the
environmental impact of a whole recipe rather then the individual
ingredients in presented in [@Toledo2020].

## Ingredient linking

Various food description, classification and coding systems have been
developed by different countries and organisations
[@Brown2017; @Ireland2000]. Therefore, the International Network of Food
Data Systems (FAO/INFOODS) prepared guidelines for food matching, that
is, the process of linking food consumption data to food databases
[@Barbara2012]. An example of a coding system is called FoodEx2, which
stands for 'Food classification and description for Exposure assessment'
[@Food2014]. The system has a hierarchical structure of parent-child
relationships between food groups, categories and items. All terms have
a unique alphanumerical code, a name and description in English
language. Ingredients encoding at the sentence level is presented in
[@Zan2020]. The ingredient lines in the recipe list are encoded with
BERT to obtain an ingredient embedding, which is concatenated with the
title and instruction embeddings to a recipe embedding vector. FoodIE
[@Popovski2019a] is used in [@Popovski2019] to link food entities to
semantic tags in the 'Food and drink' category of the Hansard corpus
[^5]. NEL based on string match or DBpedia spotlight [@Daiber2013] is
presented in [@VanErp2018].

## Analysis of related work

Regarding recipe data, our analysis shows that completely manually
annotated data sets are rare ([@Greene2015] only). Manual annotation of
a subset is a more common approach, either for training or evaluation of
ingredient extraction models. Regarding description recognition,
lexicon-based, rule-based and learning-based sequence tagging approaches
have been published. Comparing the performance of the three approaches
is complex since different data sets, definitions of entities, and
ground truth labels are used. In addition, we note that the rule-based
approach of FoodIE [@Popovski2019a] is insightful due to transparency in
used features. Regarding sustainability food data and linking, string
matching and classification approaches to linking have been published.
In addition, standardization of food coding systems is an ongoing
process.

# Methodology {#sec:meth}

::: figure*
:::

We take an empirical approach to the the task of linking online Dutch
recipes to a sustainability KB. We begin by formalizing the problem, we
then describe the proposed model and experimental design.

## Problem formulation {#subsec:framework}

Our starting point is a conceptualization of a recipe as an object
denoted by $R$. A recipe object $R$ has two attributes. The first
attribute is a set $D$ that contains descriptions of ingredients and/or
utensils required to produce the dish. The second attribute of recipe
$R$ is a footprint $F$ which we define as the distribution of GHG
emission per unit mass allocated to the dish. More specifically,
footprint $F$ is defined as the 5^th^ percentile $F^{5th}$, mean
$F^{mean}$ and 95^th^ percentile $F^{95th}$ of the distribution of GHG
emission per unit mass. Other attributes of a recipe, such as the
preparation steps, are not considered since they are not in the scope of
this work. Based on the formulation described in this section, the
problem addressed in our research question is formalized as: *Given a
set of descriptions $D$ of recipe $R$, can we make a conservative
footprint prediction $\hat{F}$ for the actual footprint $F$ of recipe
$R$?* We consider a footprint prediction conservative if
$\hat{F}^{5th} \le F^{5th}$ and $\hat{F}^{95th} \ge F^{95th}$.

## Model {#subsec:model}

Our model is structured according to the sub-questions of our research
question, therefore we define four tasks in our model: 1) **tagging**,
2) **classification**, 3) **conversion** and 4) **emission** allocation.
Besides these four tasks, we define an quantified ingredient object
denoted by $I_q$. A quantified ingredient object $I_q$ is a
conceptualization of an ingredient or utensil required to produce the
dish. We assume that each description $d$ in the set of descriptions $D$
is an attribute of a quantified ingredient object $I_q$. Besides a
description $d$, a quantified ingredient object $I_q$ has three other
attributes: 1) a food class denoted by $C$, 2) a mass denoted by $M$ and
3) a distribution of GHG emission per unit mass denoted by $E$. The
objective of the four defined tasks (tagging, classification, conversion
and emission allocation) is to make a prediction of the attributes of a
quantified ingredient object $I_q$ based its description attribute $d$.
Our modeling decisions per task are explained in more detail in the
following paragraphs.

For the **tagging** task, we assume that each item $d$ in the set of
descriptions $D$ is a sequence of tokens. Each token in each description
$d$ represent either the food class $C$ or the mass $M$ of its
quantified ingredient object $I_q$. If a token represents mass $M$, it
can either have label $U$ which stands for unit measure or $Q$ which
stands for quantity. If a token represents the food class $C$, it has
label $I$ which stands for ingredient. In our model, each sequence of
tokens $d$ is enriched to a sequence of labeled tokens, which we denote
by $d^{tag}$. The objective of the tagging task is to predict the labels
in $d^{tag}$. The predicted sequence of labeled tokens is denoted by
$\hat{d}^{tag}$, a set of sequences of labeled tokens is denoted by
$\hat{D}^{tag}$.

The next task in our model is **classification**. We assume that a
prediction of the food class can be made based on the token(s) labelled
as ingredient $I$. We denote the prediction of the food class as
$\hat{C}$. It is important to note that the tokens used in the
classification task depend on the output of the tagging task.

The next task in our model is **conversion**. We assume that a
prediction of the mass can be made based on the token(s) labeled as
quantity $Q$ or unit $U$ and based on the predicted food class
$\hat{C}$. We denote the prediction of the mass as $\hat{M}$. It is
important to note that the output of the conversion task depend on both
the output of the tagging and classification task.

The final task in our model is **emission** allocation. The first
assumption is that a prediction of the GHG emission can be made based on
the predicted food class $\hat{C}$ and predicted mass $\hat{M}$. We
denote the prediction of the GHG emission as $\hat{E}$. The second
assumption is that a prediction of the footprint of a recipe can be made
based on the mass weighted average of the predicted GHG emission
$\hat{E}$ for all quantified ingredients represented by description $d$
in set of descriptions $D$. We denote the prediction of footprint as
$\hat{F}$.

## Experimental design {#subsec:exp}

Our experimental design operationalizes our model in the form of a data
system. Figure [\[fig:exp_Setup\]](#fig:exp_Setup){reference-type="ref"
reference="fig:exp_Setup"} show a high level graphical representation of
the data system in which we define three independent variables and four
evaluation metrics (or dependent variables). Inputs and output are
connected via a number of data processing steps that we define as the
pipeline. Since the pipeline contains trainable models, we define two
cycles: 1) the train & validation cycle and 2) the test cycle. The code
of this data system can be found on Github[^1].

### Independent variables {#subsubsec:variables}

The three independent variables and the values they can take are as
follows. The first variable is tagging model $T$. It represents which
tagging model is used in the configuration of the pipeline. The three
possible values for $T$ are $T_{sti}$, $T_{num}$ and $T_{pos}$. The next
variable is food classification set $L_C$. It represents the unique
labels that can be assigned to the food class $C$ of a quantified
ingredient objects $I_q$ in the configuration of the pipeline. The five
possible values for $L_C$ are $E$, $C$, $P$, $M$, $H$ each of which
represent a set of classification labels from the FoodEx2 system with a
maximum granularity level. The third variable is food classification
threshold $P_t$. It represents a threshold to a softmax value. The
prediction class $\hat{C}$ is based on the output class $\hat{C}_{max}$
with the highest softmax value $P_{max}$. If $P_{max} < P_t$, than
$\hat{C}$ is set to $NFOOD$. If $P_{max} \ge P_t$, than $\hat{C}$ is
selected by mapping $\hat{C}_{max}$ to one the classes in $L_C$. The
nine possible values for $P_t$ are $0.1$, $0.2$, $0.25$, $0.3$, $0.4$,
$0.45$, $0.5$, $0.6$ and $0.7$.

### Pipeline

The input of the pipeline is a set of recipe data, which was scraped
from the website of 24Kitchen[^2] in April 2021. It consists of 7,097
recipes which have 82,263 ingredient/utensil descriptions which in total
comprise of 240,131 tokens. All tokens are annotated in the HTML-code as
'quantity', 'unit' or 'ingredient', except for 56 tokens which we choose
to label as 'others'. A summary of the exploratory data analysis is
provided in Appendix [\[app:eda\]](#app:eda){reference-type="ref"
reference="app:eda"}.

The **preprocess** component of the pipeline takes in the recipe data
and processes it in three steps: first tokenization, second
Part-Of-Speech (POS) tagging and third cleaning (removal of punctuation
and spaces).

The **tagging** component takes in the tokens of a description $d$,
engineers and scales the features that form the input of the tagging
model $T$. The output of the tagging model is a predicted tag label for
each token.

The **classification** component selects the tokens tagged as ingredient
$I$, translates them to English language[^3] and feeds them to a
classification model that is pretrained by the European Food Safety
Agency (EFSA) [^4]. The prediction class $\hat{C}$ is based on the
output class $\hat{C}_{max}$ with the highest softmax value $P_{max}$.
If $P_{max} < P_t$, than $\hat{C}$ is set to $NFOOD$. If
$P_{max} \ge P_t$, than $\hat{C}$ is selected by mapping $\hat{C}_{max}$
to one the classes in $L_f$. The mapping is based on the hierarchy of
the FoodEx2 coding system. The five levels of granularity are from low
to high: 1) Hierarchy Term (H), 2) Aggregation Term (M), 3) Non-specific
Term (P), 4) Core Term (C) and 5) Extended Term (E).

The **conversion** component selects the tokens tagged as quantity $Q$
or unit $U$. Tokens tagged as quantity $Q$ are converted to a numerical
value if possible. Tokens tagged as unit are looked-up in three tables.
The first table is dictionary with common units of mass, that is, 'g',
'kg', 'gr', 'gram' and 'kilo'. The second table is a dictionary with
common units of volume, that is, 'ml', 'l' and 'liter'. The third table
is an extensive database called Portie-online [@RIVM2020] with units of
measure used for food and their respective mass in grams for a food
category specified in the NEVO[^5] coding system. The NEVO code is
obtained by mapping the predicted class $\hat{C}$, which is expressed in
FoodEx2, to a NEVO code. If no tokens with tag 'unit' are present (e.g.
'2 onions'), than we assume the unit of measure is not explicitly
mentioned but does exist. In that case, we lookup the Dutch word 'stuks'
(in English 'number of items') in the Portie-online database. If a
numerical value is found for both the quantity and unit, than $\hat{M}$
is set to the product of these values. If at least one of them could not
be converted to a numerical value, than $\hat{M}$ is set to zero.

The **emission** component takes in the predicted class $\hat{C}$ and
mass $\hat{M}$. The GHG emission $\hat{E}$ is selected based on a lookup
of the predicted class $\hat{C}$ in the GHG knowledge base. If the
prediction class $\hat{C}=NFOOD$, we assume the most conservative values
in the knowledge base for the bounds, that is,
$\hat{E}^{5th}=min(E^{5th})$ and $\hat{E}^{95th}=max(E^{95th})$, and
$\hat{E}^{mean}=mean(E^{mean})$.

The **eval~tag~** component takes in the actual tagged descriptions
$D^{tag}$ and predicted tagged descriptions $\hat{D}^{tag}$. The
$F_1^{tag}$ value is calculated based on the macro averaged precision
$Pr$ and macro averaged recall $Re$ per classed as shown in Equations
[\[eq:tag_pr\]](#eq:tag_pr){reference-type="ref" reference="eq:tag_pr"},
[\[eq:tag_re\]](#eq:tag_re){reference-type="ref" reference="eq:tag_re"}
and [\[eq:tag_k1\]](#eq:tag_k1){reference-type="ref"
reference="eq:tag_k1"}.

$$\label{eq:tag}
\begin{align}
Pr = \frac{1}{3} \sum_{l \in \{Q,U,I\}} Pr_l = & \frac{1}{3} \sum_{l \in \{Q,U,I\}} \frac{TP_l}{TP_l+FP_l} \label{eq:tag_pr} \\
Re = \frac{1}{3} \sum_{l \in \{Q,U,I\}} Re_l = & \frac{1}{3} \sum_{l \in \{Q,U,I\}} \frac{TP_l}{TP_l+FN_l} \label{eq:tag_re} \\
F_1^{tag} = & \frac{2 Pr \cdot Re}{Pr + Re} \label{eq:tag_k1} 
\end{align}$$

$TP_l$, $FN_l$ and $FP_l$ respectively denote the number of True
Positive, False Negative and False Positive tag predictions of tokens
with label $l$.

The **eval~link~** component takes in predicted class $\hat{C}$,
predicted mass $\hat{M}$, set of classification labels $L_C$, and ground
truth labels $C$ and mass $M$. Since annotation has to be done manually
and our resources our limited, a subset of recipes in the test set is
selected for annotation. More details about the annotated subset of
recipes and the rationale for this selection is detailed in Appendix
[\[app:ann\]](#app:ann){reference-type="ref" reference="app:ann"}. The
classification accuracy $a_C$ and mass ratio $r_M$ are calculated as
described in Equation [\[eq:link\]](#eq:link){reference-type="ref"
reference="eq:link"}.

$$\label{eq:link}
\begin{align}
a_C = &\frac{TP}{TP+FP+TN+FN} \label{eq:link_acc} \\
r_M = &\frac{\hat{M}}{M} \label{eq:link_mr} 
\end{align}$$

### Hypothesis

Our experiment is designed to be able to test the following hypothesis.

::: hypothesis
[]{#hy:recall label="hy:recall"} Given a food classification model, a
set of food classes $L_C$ and a classification threshold $P_t$, \...

1.  \... a higher ingredient recall $Re_I$ leads to higher
    classification accuracy $a_C$

2.  \... a higher quantity recall $Re_Q$ or unit recall $Re_U$ leads to
    higher mass ratio $r_M$
:::

::: hypothesis
[]{#hy:granularity label="hy:granularity"} Given a food classification
model, a tagger model $T$ and a classification threshold $P_t$, \...

1.  \... a more granular set of food classes $L_C$ leads to lower
    predictions accuracy $a_C$

2.  \... a more granular set of food classes $L_C$ leads to higher mass
    ratio $r_M$
:::

::: hypothesis
[]{#hy:threshold label="hy:threshold"} Given a food classification
model, a set of food classes $L_C$ and a tagger model $T$, \...

1.  \... a higher classification threshold $P_t$ leads to a higher
    classification accuracy $a_C$

2.  \... a higher classification threshold $P_t$ leads to a lower mass
    ratio $r_M$
:::

### Trained models {#subsubsec:trained}

The train and validation cycle is setup to design the three taggers
represented by $T_{sti}$, $T_{num}$ and $T_{pos}$. The data is split in
three sets: train, validation and test (see Table
[1](#tabel:splitsizes){reference-type="ref"
reference="tabel:splitsizes"}). The split is made by stratified random
sampling on a recipe level based on recipe category (Main, Appetizer,
Dessert and Unknown). Stratification on recipe category is done to
prevent a bias in the model towards a specific category. Splitting on a
recipe level is done to make sure that each split has a representative
set of ingredients.

::: {#tabel:splitsizes}
                       Recipes      Ingredients           Tokens
  ------------ --------------- ---------------- ----------------
  Train          5,640 (79.5%)   65,599 (79.7%)   191,965(79.8%)
  Validation       726 (10.2%)    8,226 (10.0%)   24,131 (10.0%)
  Test             731 (10.3%)    8,438 (10.3%)   24,475 (10.2%)
  Total                  7,097           82,263          240,571

  : Size of data splits: Absolute size (relative size as percentage of
  total)
:::

All tagging models are rule-based models that are implemented by fitting
a decision tree. We used the Scikit-learn[^6] package in Python, which
uses the CART algorithm [@Dension1998] to fit trees. Feature engineering
for the decision trees is done in a three-step process. In the first
step, features are based on the exploratory data analysis (see Appendix
[\[app:eda\]](#app:eda){reference-type="ref" reference="app:eda"}). We
create six binary features for $T_{sti}$: three features that
respectively represent description length of $1$, $2$ or $3$ tokens and
three features that respectively represent token position of $0$, $1$ or
$2$. In the second step, an additional feature (on top of the features
for $T_{sti}$) is selected based on the hypothesis that tokens that are
numerical most probably represent a quantity. Therefore tagger $T_{num}$
is trained based on the six features of $T_{sti}$ and the binary feature
that describes if a token is numerical. In the third step, the tokens
that are incorrectly classified by $T_{num}$ are analysed by
Part-Of-Speech (POS) tags. The POS tags in the top 5 incorrectly
classified tokens are selected as potential additional features (shown
in subsection [\[fig:RB_error_1\]](#fig:RB_error_1){reference-type="ref"
reference="fig:RB_error_1"}). All combinations of these potential
additional features are tested, which resulted in three additional
binary features for $T_{pos}$ that lead to a performance increase
compared to $T_{num}$: the features describe if a token has 'adj', 'adp'
or 'cconj' POS tag.

Hyperparameter tuning is done as follows. The maximum tree depth and
initial random state of the decision tree models are selected as
hyperparameter to tune. A grid search is done for $T_{sti}$, $T_{num}$
and $T_{pos}$ in which all combinations of maximum tree depth of $1$,
$2$, \..., $10$ and $None$ and initial random state of $1$, $2$, \...,
$20$. Within the set of models with the highest $F_1^{tag}$-score, the
model with the lowest maximum tree depth is selected as final
configuration.

\section{Results} \label{sec:res}
This section describes and interprets the data collected during the experiment described in Section \ref{sec:meth}. To give a first impression of the output of the pipeline, we present Example \ref{ex:wijnazijn} that is correctly tagged, classified and converted and Figure \ref{fig:main_metrics} that gives an overview of the evaluation metrics for tagging, classification and conversion.

\begin{example}\label{ex:wijnazijn}
The tokens in description $d$ `20 ml rode wijnazijn' (in English `20 ml red wine vinegar') are respectively tagged as quantity, unit, ingredient, ingredient. The quantity and unit are converted to 20 grams and the ingredient is classified as FoodEx2 code `A044M' that represents `Vinegar, wine'. 
\end{example}

\begin{figure}[H]
%\includegraphics[width=6cm]{example-image-duck}
\input{Images/pgf/main_metrics.pgf}
\caption{Overview of tagging and linking evaluation metrics for annotated recipes in test set, maximum per tagger}
\label{fig:main_metrics}
\end{figure}

Our two main observations for Figure \ref{fig:main_metrics} are: 1) the $F_1^{tag}$-score increases from $T_{sti}$ to $T_{num}$ and from $T_{num}$ to $T_{pos}$ (which is further analyzed in Subsection \ref{subsec:TVcycle}), and 2) the maximum classification accuracy $a_C$ and mass ratio $r_M$ do not change from $T_{sti}$ to $T_{num}$ and increase from $T_{num}$ to $T_{pos}$ (which is further analyzed in Subsection \ref{subsec:TESTcycle} and \ref{subsec:TESTcycle} respectively). 

\subsection{Tagging} \label{subsec:TVcycle}
The confusion matrix of $T_{sti}$ over the validation set is shown in Table \ref{tabel:RB_per_1}. Our main observation from this table is that there are no quantity-tokens tagged as unit and vise versa.    

\begin{table}[H]
\begin{tabular}{ll|rrrr|rr}
\toprule
           &             & \multicolumn{6}{l}{Prediction label} \\
           &             &     qty & ingr. & others &  unit &    sum & $Re_l$ \\
\midrule
\multirow{6}{*}{\rotatebox[origin=c]{90}{Actual label}} & qty &       6,893 &         14 &      0 &     0 &   6,907 &       99.8\% \\
           & ingr. &        482 &      10,772 &      0 &   901 &  12,155 &       88.6\% \\
           & oth. &         46 &          6 &      0 &     4 &     56 &        0\% \\
           & unit &          0 &         22 &      0 &  4,991 &   5,013 &       99.6\% \\
\cmidrule{2-8}
           & sum &       7,421 &      10,814 &      0 &  5,896 &  24,131 &       96.0\%\textsuperscript{*} \\
           & $Pr_l$  &         92.9\% &         99.6\% &      0\% &    84.7\% &     92.4\%\textsuperscript{*} &       94.2\%\textsuperscript{**} \\
\bottomrule
\end{tabular}
\caption{Confusion matrix over validation set for $T_{sti}$. *) is macro averaged recall $Re$ / precision $Pr$, **) is $F_1^{tag}$-score}
\label{tabel:RB_per_1}
\end{table}

Table \ref{tabel:RB_per_2} shows the confusion matrix of $T_{num}$ over the validation set. Our first observation from this table is that the additional `like-num' feature lead to an increase of $F_1^{tag}$-score to 96\% (increase of 2\% compared to $T_{sti}$ shown in Table \ref{tabel:RB_per_1}). The second observation is that the number of ingredient-tokens tagged as quantity is $6$, a reduction of $476$ compared to Table \ref{tabel:RB_per_1} which is illustrated by Example \ref{ex:olijfolie1}. 

\begin{table}[H]
\begin{tabular}{ll|rrrr|rr}
\toprule
           &             & \multicolumn{6}{l}{Prediction label} \\
           &             &     qty & ingr. & others &  unit &    sum & $Re_l$ \\
\midrule
\multirow{6}{*}{\rotatebox[origin=c]{90}{Actual label}} & qty &       6,893 &         14 &      0 &     0 &   6,907 &       99.8\% \\
           & ingr. &          6 &      11,248 &      0 &   901 &  12,155 &       92.5\% \\
           & oth. &         23 &         29 &      0 &     4 &     56 &        0\% \\
           & unit &          0 &        22 &      0 &  4,991 &   5,013 &       99.6\% \\
\cmidrule{2-8}
           & sum &       6,922 &      11,313 &      0 &  5,896 &  24,131 &       97.3\%\textsuperscript{*} \\
           & $Pr_l$ &         99.6\% &         99.4\% &      0\% &    84.7\% &     94.6\%\textsuperscript{*} &       95.9\%\textsuperscript{**} \\
\bottomrule
\end{tabular}
\caption{Confusion matrix over validation set for $T_{num}$. *) is macro averaged recall $Re$ / precision $Pr$, **) is $F_1^{tag}$-score}
\label{tabel:RB_per_2}
\end{table}

Table \ref{tabel:RB_per_3} shows the confusion matrix of $T_{pos}$ over the validation set. Our first observation from this table is that the additional POS features lead to an increase of $F_1^{tag}$-score to 98\% (increase of 2\% compared to $T_{num}$ in Table \ref{tabel:RB_per_2}). Our second observations from Table \ref{tabel:RB_per_3} is that the number of ingredient-tokens tagged as unit is $239$, a reduction $662$  compared to Table \ref{tabel:RB_per_2} which is illustrated by Example \ref{ex:olijfolie1}. Thirdly, the number of unit-tokens tagged as ingredient is $209$, an increase of $187$ compared to Table \ref{tabel:RB_per_2}.

\begin{table}[H]
\begin{tabular}{ll|rrrr|rr}
\toprule
           &             & \multicolumn{6}{l}{Prediction label} \\
           &             &     qty & ingr. & others &  unit &    sum & $Re_l$ \\
\midrule
\multirow{6}{*}{\rotatebox[origin=c]{90}{Actual label}} & qty &       6,893 &         14 &      0 &     0 &   6,907 &       99.8\% \\
           & ingr. &          6 &      11,910 &      0 &   239 &  12,155 &       98.0\% \\
           & oth. &         23 &         30 &      0 &     3 &     56 &        0\% \\
           & unit &          0 &         209 &      0 &  4,804 &   5,013 &       95.8\% \\
\cmidrule{2-8}
           & sum &       6,922 &      12,163 &      0 &  5,046 &  24,131 &       97.9\%\textsuperscript{*} \\
           & $Pr_l$ &         99.6\% &         97.9\% &      0\% &    95.2\% &     97.6\%\textsuperscript{*} &       97.7\%\textsuperscript{**} \\
\bottomrule
\end{tabular}
\caption{Confusion matrix of validation set for tagger $T_{pos}$. *) is class weighted recall/precision-score, **) is $K_1^{tag}$-score}
\label{tabel:RB_per_3}
\end{table}

\begin{example} \label{ex:olijfolie1}
The five tokens in description $d$ `olijfolie om in te bakken' (in English `olive oil for frying purposes') are respectively tagged as `quantity', `unit', `ingredient', `ingredient', `ingredient' by $T_{sti}$ and as `ingredient', `unit', `ingredient', `ingredient', `ingredient' by $T_{num}$. All tokens are tagged as `ingredient' by $T_{pos}$. Since the ground truth labels of all tokens are `ingredient', $T_{sti}$ tags $3/5$ correct,  $T_{num}$ tags $4/5$ correct and $T_{pos}$ tags $5/5$ correct.   
\end{example}

\subsection{Classification} \label{subsec:TESTcycle}
The classification accuracy $a_C$ for $T_{pos}$ per set of food classes $L_C$ and classification threshold $P_t$ is shown in Figure \ref{fig:class_per}. Our first main observation is the increasing trend in classification accuracy $a_C$ for each set of food classes $L_C$ from classification threshold $P_t$ of $10\%$ to $25\%$ (as illustrated by Example \ref{ex:staafmixer}) and the decreasing trend in classification accuracy $a_C$ for each set of food classes $L_C$ from classification threshold $P_t$ of $30\%$ to $70\%$ (as illustrated by Example \ref{ex:bladpeterselie})  

\begin{figure}[H]
%\includegraphics[width=6cm]{example-image-duck}
\input{Images/pgf/class_acc.pgf}
\caption{Classification accuracy per classification granularity for tagger $T_{pos}$}
\label{fig:class_per}
\end{figure}

\begin{example} \label{ex:staafmixer}
The single token in description $d$ `staafmixer' (in English `hand mixer') is tagged as `ingredient' by all taggers, and classified to `Hard candies' (FoodEx2- code: `A034X') for $P_t=10\%$ and to `NFOOD' (which is the correct class) for $P_t=25\%$.
\end{example}

\begin{example} \label{ex:bladpeterselie}
The tokens in description $d$ `0.5 bosje bladpeterselie' (in English `0.5 grove flat-leaf parsley') are respectively tagged as `quantity', `unit' and `ingredient' by all taggers. The ingredient-token is classified as `Parsley' (FoodEx2- code: `A00YE') for $P_t=30\%$ (which is the correct class) and to `NFOOD'  for $P_t=70\%$.
\end{example}

Our second observation in Figure \ref{fig:class_per} is that the classification accuracy $a_C$ over all classification threshold $P_t$ values is lowest for the set of food classes $L_C$ with highest granularity and highest for the set of food classes $L_C$ with lowest granularity, as illustrated by Example \ref{ex:biefstuk}.

\begin{example} \label{ex:biefstuk}
The tokens in description $d$ `400 g biefstuk' (in English `400 g beefsteak') are respectively tagged as `quantity', `unit' and `ingredient' by all taggers. The ingredient-token is classified as `Marinated meat' (FoodEx2- code: `A0EYQ') on a high granularity level (indicated with $E$), where the correct class is `Cow, ox or bull fresh meat' (FoodEx2- code: `A01QX'). The same ingredient-token is classified as `Meat and meat products' (FoodEx2- code: `A01QR') on a low granularity level (indicated with $H$), which is correct.
\end{example}

Our third observation in Figure \ref{fig:class_per} is that the classification accuracy $a_C$ for the set of food classes $L_C$ with granularity $C$ is higher than for the set of food classes $L_C$ with granularity $P$, which is noteworthy since $C$ represents a higher granularity level than $P$.

The classification accuracy $a_C$ for classification threshold $P_t=25\%$ per set of food classes $L_C$ against ingredient recall $Re_I$ is shown in Figure \ref{fig:class_error}. Our main observation is that the classification accuracy $a_C$ does not change when ingredient recall $Re_I$ increases from approx. $93.0\%$ to approx. $95.5\%$, but does increase when ingredient recall increases from approx. $95.5\%$ to approx. $99.5\%$, as illustrated by Example \ref{ex:olijfolie}.      

\begin{figure}[H]
%\includegraphics[width=6cm]{example-image-duck}
\input{Images/pgf/class_acc_breakdown.pgf}
\caption{Classification accuracy $a_C$ for classification threshold $P_t=25\%$ per set of food classes $L_C$ against ingredient recall $Re_I$}
\label{fig:class_error}
\end{figure}

\begin{example} \label{ex:olijfolie}
(continuation of Example \ref{ex:olijfolie1}) The ingredient recall $Re_I$ in ingredient description `olijfolie om in te bakken' (in English `olive oil for frying purposes')  increases from $3/5$ for $T_{sti}$, to $4/5$ for $T_{num}$ and to $5/5$ for $T_{pos}$. The predicted food classes are `Snacks other than chips and similar' (FoodEx2- code: `A06HL') for $T_{sti}$, `Olives for oil production' (FoodEx2- code: `A016M') for $T_{num}$ and `Olive oils' (FoodEx2- code: `A036P') for $T_{pos}$. Only the latter food class is correct and contributes to a higher classification accuracy $a_C$.
\end{example}

\subsection{Conversion} \label{subsec:conversion}

The mass ratio $r_M$ for $T_{pos}$ per set of food classes $L_C$ and classification threshold $P_t$ is shown in Figure \ref{fig:conv_per}. Our first observation is the trend of a decreasing mass ratio $r_M$ from classification threshold $P_t$ of $10\%$ to $45\%$ for all sets of food classes $L_C$, as illustrated by Example \ref{ex:tomaten}. 

\begin{figure}[H]
%\includegraphics[width=6cm]{example-image-duck}
\input{Images/pgf/mass_ratio.pgf}
\caption{Mass ratio $r_M$ per set of food classes $L_C$ and classification threshold $P_t$ for $T_{pos}$}
\label{fig:conv_per}
\end{figure}

\begin{example} \label{ex:tomaten} 
The tokens in ingredient description `2 tomaten' (in English `2 tomatoes') are respectively tagged as `quantity' and `ingredient'  by all taggers. The mass is predicted at $370$ grams for $P_t=10\%$, based on the assumption that the unit is 'items' (since it is not explicitly states in the ingredient description) and the predicted ingredient class of `Tomatoes' (FoodEx2- code: `A0DMX'). In this case the mass can be predicted since the mass of one tomato item can be looked-up. The mass cannot be predicted for $P_t=20\%$ since the predicted food class is `NFOOD', which prevents a look-up of the mass of an item. 
\end{example}

Our second observation in Figure \ref{fig:conv_per} is that the gap in mass ratio $r_M$ between the set of food classes $L_C$ with the lowest granularity (indicated as $H$) and the other sets, as illustrated by Example \ref{ex:courgette}. 

\begin{example} \label{ex:courgette}
The tokens in ingredient description `1 courgette' (in English `1 courgette') are respectively tagged as `quantity' and `ingredient' by all taggers. The mass is predicted at $93$ grams for the set of food classes $L_C$ with the highest granularity (indicated as $E$). As in Example \ref{ex:tomaten}, the mass is based on the assumption that the unit is 'items' and the predicted ingredient class of `Courgettes' (FoodEx2- code: `A00JR'). For the set of food classes $L_C$ with the lowest granularity (indicated as $H$), the predicted ingredient class of `Cucurbits fruiting vegetables' (FoodEx2- code: `A00JK'). The mass of one item cannot be looked-up for such a broad category.
\end{example}

Our third observation in Figure \ref{fig:conv_per} is that the drop in mass ratio $r_M$ from classification threshold $P_t$ of $45\%$ to $50\%$ for all sets of food classes $L_C$. The drop is explained (see Figure \ref{fig:conv_error}) by two ingredients that account for about $25\%$ of the predicted mass $\hat{M}$. One is `water' (in English `water') and the other is `ui' (in English `onion'), both are classified as `NFOOD' starting from classification threshold $P_t=50\%$.

\begin{figure}[H]
%\includegraphics[width=6cm]{example-image-duck}
\input{Images/pgf/mass_ratio_breakdown2.pgf}
\caption{Predicted mass $\hat{M}$ for tagger $T_{pos}$ and classification granularity $E$}
\label{fig:conv_error}
\end{figure}

\subsection{Emission allocation} \label{subsec:emissionall}

The footprint for ten annotated recipes is shown in Figure \ref{fig:emission_subset}. Our first observation is that the predicted lower bound $\hat{F}^{5th}$ is not conservative (that is, not lower or equal than the actual lower bound $F^{5th}$) for all recipes, as illustrated by Example \ref{ex:Karbonades}.

\begin{figure}[H]
%\includegraphics[width=6cm]{example-image-duck}
\input{Images/pgf/emission_comparison.pgf}
\caption{Predicted vs annotated footprint with tagger $T_{pos}$, classification granularity $E$ and classification thld. $0.25$}
\label{fig:emission_subset}
\end{figure}

\begin{example}\label{ex:Karbonades}
Recipe with ID $5983$ describes the ingredients for the main course `Karbonades met geroosterde zoete aardappelsalade' (in English `Pork chops with roasted sweet potato salad'). Predicted lower bound $\hat{F}^{5th}$ is higher than the actual lower bound $F^{5th}$ since the ingredient described as `4 karbonades' (in English `4 pork chops'). It is classified as `Calf fresh meat' (FoodEx2-code: `A01QY') with lower bound $E^{5th}=38$ but annotated as `Pig fresh meat' (FoodEx2-code: `A01RG') with lower bound $E^{5th}=7$.  
\end{example}

Second observation from Figure \ref{fig:emission_subset} is that the predicted upper bound $\hat{F}^{95th}$ is not conservative (that is, not higher or equal than the actual upper bound $F^{95th}$) for all recipes, as illustrated by Example \ref{ex:biefstuk2}.  

\begin{example}\label{ex:biefstuk2}
Recipe with ID $435$ describes the ingredients for the main course `Biefstuk Chimichurri' (in English `Beefsteak Chimichurri'). Predicted upper bound $\hat{F}^{95th}$ is lower than the actual upper bound $F^{95th}$ since the ingredient described as `400 g biefstuk' (in English `400 g beefsteak'). It is classified as `Marinated meat' (FoodEx2-code: `A0EYQ') with upper bound $E^{5th}=24$ but annotated as `Cow, ox or bull fresh meat' (FoodEx2-code: `A01QX') with upper bound $E^{5th}=269$.  
\end{example}

# Discussion {#sec:eva}

In this section we will reflect on our work by describing the limitation
of our methodology and the implications of our results.

## Limitations

We discuss the limitations of our methodology in terms of reliability,
validity and generalizability. First, we reflect on reliability by
considering the size of the annotated data set. The tagging results are
reliable since they are tested on over 700 annotated recipes (see Table
[\[tabel:splitsizes\]](#tabel:splitsizes){reference-type="ref"
reference="tabel:splitsizes"}). On the other hand, the classification
and conversion results are less reliable since they are tested on only
10 annotated recipes, which contain 120 ingredients. The FoodEx2 coding
system has approximately 4,000 unique food classes, so more reliable
classification and conversion results can be obtained with an annotated
set containing multiple ingredients per food class.

Secondly, we reflect on validity by considering the sources of the input
data and assumptions. The GHG emission KB used in this study is not
published at the submission date of this work, therefore the validity of
the GHG emission values is unknown. We only use the GHG emission values
from the KB to illustrate how the pipeline could be used for recipe
footprint assessment, without claiming that the emission values are
valid. Then, we consider the assumptions and resources used in the
conversion task. The mapping, from one food coding system (NEVO) to
another food coding system (FoodEx2), that we perform in the conversion
task is not based on published work. Consequently, the validity of this
mapping is unknown. Also, the assumption that a unit of 'stuks' (in
English 'number of items') can be assumed when the unit is not
specified, is not validated. Furthermore, the annotation in terms of
FoodEx2 and mass is done by one of the authors. Since this author has no
expertise in the FoodEx2 coding system or mass annotation, the validity
of ground truth labels is unknown.

Thirdly, we reflect on generalizability. For tagging, the
$F_1^{tag}$-scores over the train and validation set are separately
measured and found to be consistent (see Appendix
[\[app:hyper\]](#app:hyper){reference-type="ref"
reference="app:hyper"}). As a result, the tagging results are considered
generalizable to recipe data sets with similar characteristics as our
data set. Nevertheless, generalizability towards data sets from other
websites or other languages is not tested. Also, generalizability
towards recipe types, that are relatively underrepresented in the train
set, is not tested. For classification and conversion, we have no data
on which we can base any claims about generalizability of our results.
However, since the set of annotated recipes is small with only 120
ingredients compared to 4,000 different classes (see Appendix
[\[app:ann\]](#app:ann){reference-type="ref" reference="app:ann"}), the
generalizability is assumed to be limited.

## Implications

The implications of our results are structured according to the four
tasks defined in our methodology: 1) **tagging**, 2) **classification**,
3) **conversion** and 4) **emission** allocation. To start with
**tagging**, we argue rule-based tagging is transparent, effective and
efficient for the following reasons. It is transparent since the
eventual decision tree is human readable and understandable (see
Appendix [\[app:trees\]](#app:trees){reference-type="ref"
reference="app:trees"}). Rule-based tagging is effective since an
$F_1^{tag}$-score of 98% is reached, which is considered high.
Rule-based tagging is efficient since only ten input features are used.
Furthermore, the results imply that specific features are effective to
distinguish tokens with specific tag labels. For instance, the two
features description length and token position. Since no quantity tokens
are tagged as units or vise versa when these two feature are used (see
Table [\[tabel:RB_per_1\]](#tabel:RB_per_1){reference-type="ref"
reference="tabel:RB_per_1"}), they are effective to distinguish quantity
tokens from unit tokens. Also, the numerical-like feature can
effectively distinguish quantity tokens from ingredient tokens, since
the addition of this numerical-like feature led to a reduction of the
number of ingredient tokens labeled as quantities or vise versa (see
Table [\[tabel:RB_per_2\]](#tabel:RB_per_2){reference-type="ref"
reference="tabel:RB_per_2"}). In addition, the POS features can
effectively distinguish unit tokens from ingredient tokens, since the
addition of these POS features led to a reduction of ingredient tokens
labeled as unit tokens or vise versa (Table
[\[tabel:RB_per_3\]](#tabel:RB_per_3){reference-type="ref"
reference="tabel:RB_per_3"}).

Secondly, the implications regarding **classification** are discussed
based on Hypothesis [\[hy:recall\]](#hy:recall){reference-type="ref"
reference="hy:recall"}a,
[\[hy:granularity\]](#hy:granularity){reference-type="ref"
reference="hy:granularity"}a and
[\[hy:threshold\]](#hy:threshold){reference-type="ref"
reference="hy:threshold"}a. Our results do not support Hypothesis
[\[hy:recall\]](#hy:recall){reference-type="ref"
reference="hy:recall"}a, which states a higher ingredient recall $Re_I$
leads to a higher classification accuracy $a_C$. Namely, similar
classification accuracy values $a_C$ are observed in pipeline
configurations with a different ingredient recall $Re_I$ (see Figure
[\[fig:conv_error\]](#fig:conv_error){reference-type="ref"
reference="fig:conv_error"}). A potential explanation could be that all
ingredient description tokens need to be recalled for correct
classification (see Example
[\[ex:olijfolie\]](#ex:olijfolie){reference-type="ref"
reference="ex:olijfolie"}). Also, our results do not support Hypothesis
[\[hy:granularity\]](#hy:granularity){reference-type="ref"
reference="hy:granularity"}a, which states that a more granular set of
food classes $L_C$ leads to a lower prediction accuracy $a_C$. We did
observe scenarios, in which a more granular set of food classes $L_C$
led to a lower prediction accuracy $a_C$. However, we also observed the
opposite (see Figure
[\[fig:class_per\]](#fig:class_per){reference-type="ref"
reference="fig:class_per"}). A potential explanation for a higher
prediction accuracy $a_C$, given a less granular set of food classes, is
provided in Example [\[ex:biefstuk\]](#ex:biefstuk){reference-type="ref"
reference="ex:biefstuk"}. A potential explanation for a lower prediction
accuracy $a_C$, given a less granular set of food classes, could be the
irregularities in the FoodEx2 hierarchy. Additionally, our results do
not support Hypothesis
[\[hy:threshold\]](#hy:threshold){reference-type="ref"
reference="hy:threshold"}a, which states that a higher classification
threshold $P_t$ leads to a higher classification accuracy $a_C$. We
observe two trends, one in which classification accuracy $a_C$ increases
for higher classification threshold values up to $P_t=0.25$, and a
decreasing trend for a further increasing classification threshold (see
Figure [\[fig:class_per\]](#fig:class_per){reference-type="ref"
reference="fig:class_per"}). These two trends could be explained by
classifying utensils correctly as non-food or by classifying food
incorrectly as non-food (see Examples
[\[ex:staafmixer\]](#ex:staafmixer){reference-type="ref"
reference="ex:staafmixer"} and
[\[ex:bladpeterselie\]](#ex:bladpeterselie){reference-type="ref"
reference="ex:bladpeterselie"}).

Thirdly, the implications regarding **conversion** are discussed based
on Hypothesis [\[hy:recall\]](#hy:recall){reference-type="ref"
reference="hy:recall"}b,
[\[hy:granularity\]](#hy:granularity){reference-type="ref"
reference="hy:granularity"}b and
[\[hy:threshold\]](#hy:threshold){reference-type="ref"
reference="hy:threshold"}b. The results do not support Hypothesis
[\[hy:recall\]](#hy:recall){reference-type="ref"
reference="hy:recall"}b, which states a higher quantity or unit recall
leads to higher mass ratio $r_M$. First of all, the quantity recall
$Re_Q$ did not change over the different pipeline configurations.
Therefore we have no data to base any claims on quantity recall changes.
Furthermore, lower mass ratio $r_M$ values are observed for pipeline
configurations with a higher unit recall $Re_U$. Our results do not
support Hypothesis
[\[hy:granularity\]](#hy:granularity){reference-type="ref"
reference="hy:granularity"}b, which states that a more granular set of
food classes $L_C$ leads to a higher mass ratio $r_M$. We observed that
four sets $L_C$ with different granularity levels ('E', 'C', 'P' and
'M') score very similar in terms of mass ratio and one set ('H') scores
20-30% lower. This could be explained by the conversion from FoodEx2 to
NEVO code (see Example
[\[ex:courgette\]](#ex:courgette){reference-type="ref"
reference="ex:courgette"}). In addition, our results do not support
Hypothesis [\[hy:threshold\]](#hy:threshold){reference-type="ref"
reference="hy:threshold"}b, which states that a higher classification
threshold $P_t$ leads to a lower mass ratio $r_M$. This can be explained
by the fact that non-food items, that is, kitchen utensils, do not
contribute to a recipe's mass. Therefore no gain in mass ratio can be
reached by classifying ingredient descriptions as non-food, which is
what the threshold $P_t$ does.

Fourth, the implications regarding **emission** allocation. Our design
decision, to assume the most conservative GHG emission values in the KB
if the prediction class $\hat{C}=NFOOD$, is no guarantee for
conservative footprint $\hat{F}$ assessment (see Figure
[\[fig:emission_subset\]](#fig:emission_subset){reference-type="ref"
reference="fig:emission_subset"}, and Examples
[\[ex:Karbonades\]](#ex:Karbonades){reference-type="ref"
reference="ex:Karbonades"} and
[\[ex:biefstuk2\]](#ex:biefstuk2){reference-type="ref"
reference="ex:biefstuk2"}). Both examples show that the conservative
assumption did not compensate for the error in footprint $\hat{F}$
caused by incorrect classification of ingredients that account for a
relatively high share of the recipe's total mass.

# Conclusion {#sec:conc}

We assessed the footprint of recipes in terms of GHG emission that can
be allocated to its ingredients. We developed and empirically studied a
pipeline to link Dutch recipes to a GHG emission KB which is structured
by a food coding system called FoodEx2. We conclude that, although all
sub-tasks are executed successfully to a large or very large extent,
automated recipe footprint assessment can be done to a limited extent.
Therefore, our pipeline can be considered a proof-of-technology, but not
a proof-of-concept. We have four arguments, one per sub question as
posed in Section [\[sec:intro\]](#sec:intro){reference-type="ref"
reference="sec:intro"}, to support our conclusion. Firstly, descriptions
of ingredients, quantities and units can be recognized to a very large
extent given our observed $F_1^{tag}$-scores up to 98%. Secondly,
recognized ingredient descriptions can to a large extent be correctly
linked to a FoodEx2 coded GHG KB, as our 10-recipe test case resulted in
a classification accuracy $a_C$ up to 77%. Thirdly, recognized quantity
and unit descriptions can to a very large extent be linked to a GHG KB,
as our 10-recipe test case showed that descriptions of quantities and
units can be converted to numerical values with a mass ratio $r_M$ up to
97%. Lastly, we argue for a proof-of-technology rather than a
proof-of-concept because our pipeline did not always result in
conservative footprint predictions $\hat{F}$.

Suggestions for future work are as follows. First, a requirement study
could be done to specify what the GHG emission precision requirements
are for use cases such as footprint trend monitoring or sustainable
recipe recommendation. Secondly, a resource study could be done to
select a food classification system and corresponding GHG emission KB
that enables footprint assessment that meets the requirements specified
in the requirement study mentioned earlier. Ideally, a distinction could
be made between a classification system and KB that would be
sufficiently precise assuming a) perfect prediction and b) conservative
prediction. The latter could be used for our third suggestion, that is,
to design and/or fine-tune a pipeline that can predict conservative GHG
emission values that meet the requirements of the resource study
mentioned earlier. A public recipe data set with golden standard would
be a required asset to enable comparison between different pipeline
designs or configurations.

# Acknowledgements

Throughout the writing of this thesis, I have received a great deal of
support. I would like to express my gratitude to my supervisors Prof.
Dr. Paul Groth (University of Amsterdam) and Dr. Marieke van Erp (KNAW
Humanities Cluster DHLab / KNAW HuC DHLab) for their guidance and
feedback. In addition, I would like to thank Christian Reynolds (Centre
for Food Policy City, University of London), Marja Beukers
(Rijksinstituut voor Volksgezondheid en Milieu), and Carsten Behring
(European Food Safety Authority) for providing the required resources
for this thesis.

# List of symbols {#app:los}

## Problem formulation and model

  ----------------- -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  $R$               recipe object
  $d$               description of ingredient or utensil, sequence of tokens
  $D$               set of descriptions $d$
  $d^{tag}$         description of ingredient or utensil, sequence of labeled tokens
  $\hat{d}^{tag}$   prediction for sequence of labeled tokens $d^{tag}$
  $D^{tag}$         set of sequences of labeled tokens $d^{tag}$
  $\hat{D}^{tag}$   set of predictions for sequences of labeled tokens $d^{tag}$
  $U$               token label which stands for 'unit measure'
  $Q$               token label which stands for 'quantity'
  $I$               token label which stands for 'ingredient'
  $F$               recipe footprint defined as 5^th^ percentile $F^{5th}$, mean $F^{mean}$ and 95^th^ percentile $F^{95th}$ of the distribution of Green House Gas emission per unit mass allocated to a dish \[kgCO2eq/kg\]
  $\hat{F}$         prediction for recipe footprint $F$
  $I_q$             quantified ingredient object
  $C$               food class
  $\hat{C}$         prediction for food class
  $M$               food mass
  $\hat{M}$         prediction for food mass
  $E$               5^th^ percentile $E^{5th}$, mean $E^{mean}$ and 95^th^ percentile $E^{95th}$ of the distribution of Green House Gas emission per unit mass allocated to an ingredient \[kgCO2eq/kg\]
  $\hat{E}$         prediction for ingredient Green House Gas emission per unit mass $E$
  ----------------- -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## Experimental design

  ------------- --------------------------------------------------------------------------------------------
  $T$           tagging model $T \in \{T_{sti}, T_{num}, T_{pos}\}$
  $T_{sti}$     tagging model with description token count and position as features
  $T_{num}$     tagging model with features of $T_{sti}$ plus feature that describes if token is numerical
  $T_{pos}$     tagging model with features of $T_{num}$ plus feature that describes Part-Of-Speech tag
  $L_C$         food classification set $C \in L_C$
  $P_t$         food classification threshold
  $Pr_l$        precision for token label $l \in \{Q,U,I\}$
  $Rr_l$        recall for token label $l \in \{Q,U,I\}$
  $Pr$          macro averaged precision
  $Rr$          macro averaged recall
  $F_1^{tag}$   F-1 score for tagging of tokens
  $a_C$         classification accuracy
  $r_M$         mass ratio
  ------------- --------------------------------------------------------------------------------------------

# Recipe data {#app:eda}

Our exploratory data analysis is summarized as follows. The publication
date of the recipes ranges from 2013 up to 2021, more than half of the
recipes were published in 2013. The recipes are categorized, the five
largest categories are Main ($49\%$), Unknown ($18\%$), Dessert
($11\%$), Appetizer ($8\%$) and Lunch ($6\%$). Figure
[1](#fig:eda2){reference-type="ref" reference="fig:eda2"} shows a
segmentation of all tokens in the data set by description length (i.e.
the length of the description expressed in number of tokens), token
position (first token in the description has position 0, the second
token has position 1, etc.) and dominant token label (ingredient, unit
or quantity). The percentage shown in each segment describes the share
of tokens that is labeled with the dominant label.

<figure id="fig:eda2">

<figcaption>Segmentation of tokens by description length, token position
and dominant label. The percentage shown in each segment describes the
share of tokens that is labeled with the dominant label</figcaption>
</figure>

# Emission knowledge base {#app:kb}

Figure [2](#fig:ghge1){reference-type="ref" reference="fig:ghge1"} show
an example of GHG emission for food class 'Cherry tomatoes' and all
parent nodes in the FoodEx2 hierarchy.

<figure id="fig:ghge1">

<figcaption>Example of GHG emission for food class Cherry tomatoes and
all parent nodes in the FoodEx2 hierarchy</figcaption>
</figure>

# Tagger design {#app:hyper}

Figure [3](#fig:RB_hyper_1){reference-type="ref"
reference="fig:RB_hyper_1"} shows the $F_1^{tag}$-scores for a grid
search over all combinations of hyperparameters maximum tree depth and
initial state for $T_{sti}$.

<figure id="fig:RB_hyper_1">

<figcaption>Hyperparameter tuning for tagger <span
class="math inline"><em>T</em><sub>1</sub></span></figcaption>
</figure>

Our two main interpretations of the data shown in Figure
[3](#fig:RB_hyper_1){reference-type="ref" reference="fig:RB_hyper_1"}
are that 1) the model is not over-fitted on the training set since
$F_1^{tag}$-scores over the train and validation set are close to
identical and 2) given the model type and feature set, the
$F_1^{tag}$-score seems to be bounded by a maximum value at
approximately 94% since $F_1^{tag}$-score converges towards this bound
when the maximum tree depth goes to infinity. We choose to set the
hyperparameters of $T_{sti}$ to maximum tree depth of $3$ and initial
state of $4$ since this is given the lowest model complexity at which
the accuracy is (close to) the maximum bound.

Figure [4](#fig:RB_hyper_2){reference-type="ref"
reference="fig:RB_hyper_2"} shows the $F_1^{tag}$-scores for a grid
search over all combinations of hyperparameters maximum tree depth and
initial state for $T_{num}$. Our main interpretation of the data shown
in Figure [4](#fig:RB_hyper_2){reference-type="ref"
reference="fig:RB_hyper_2"} is that the additional LIKE-NUM features
lead to an increase in the maximum bound on the $F_1^{tag}$-score since
the a maximum value is approximately 96% (increase of 2% compared to
$T_{sti}$).

<figure id="fig:RB_hyper_2">

<figcaption>Hyperparameter tuning for tagger <span
class="math inline"><em>T</em><sub><em>n</em><em>u</em><em>m</em></sub></span></figcaption>
</figure>

Figure [5](#fig:RB_error_1){reference-type="ref"
reference="fig:RB_error_1"} shows a breakdown of the incorrectly
labelled tokens by $T_{num}$ in the train set by Part-Of-Speech (POS).
Three additional binary features are selected for $T_{pos}$: the
features describe if a token has 'ADJ', 'ADP' or 'CCONJ' POS tag.

<figure id="fig:RB_error_1">

<figcaption>Error analysis of tagger <span
class="math inline"><em>T</em><sub>2</sub></span></figcaption>
</figure>

The results of hyperparameter tuning based on these additional features
is shown in Figure [4](#fig:RB_hyper_2){reference-type="ref"
reference="fig:RB_hyper_2"}. Our main interpretation of the data shown
in Figure [6](#fig:RB_hyper_3){reference-type="ref"
reference="fig:RB_hyper_3"} is that the additional POS features lead to
an increase in the maximum bound on the accuracy score since the a
maximum value is approximately 98% (increase of 2% compared to
$T_{num}$).

<figure id="fig:RB_hyper_3">

<figcaption>Hyperparameter tuning for tagger <span
class="math inline"><em>T</em><sub><em>p</em><em>o</em><em>s</em></sub></span></figcaption>
</figure>

# Decision trees {#app:trees}

## Decision tree for Tagger T~sti~

    |-- token_index_1 <= 0.4
    |  |-- token_index_0 <= 0.3
    |  |  |-- token_index_2 <= 0.6
    |  |  |  |-- class: 2
    |  |  |-- token_index_2 >  0.6
    |  |  |  |-- class: 2
    |  |-- token_index_0 >  0.3
    |  |  |-- n_tokens_1 <= 2.6
    |  |  |  |-- class: 2
    |  |  |-- n_tokens_1 >  2.6
    |  |  |  |-- class: 0
    |-- token_index_1 >  0.4
    |  |-- n_tokens_3 <= -0.0
    |  |  |-- n_tokens_2 <= 1.1
    |  |  |  |-- class: 2
    |  |  |-- n_tokens_2 >  1.1
    |  |  |  |-- class: 0
    |  |-- n_tokens_3 >  -0.0
    |  |  |-- class: 0

## Decision tree for Tagger T~num~

    |-- token_index_0 <= 0.3
    |  |-- token_index_1 <= 0.4
    |  |  |-- token_index_2 <= 0.6
    |  |  |  |-- class: 2
    |  |  |-- token_index_2 >  0.6
    |  |  |  |-- class: 2
    |  |-- token_index_1 >  0.4
    |  |  |-- n_tokens_2 <= 1.1
    |  |  |  |-- class: 2
    |  |  |-- n_tokens_2 >  1.1
    |  |  |  |-- class: 0
    |-- token_index_0 >  0.3
    |  |-- n_tokens_2 <= 1.1
    |  |  |-- NUM <= 0.5
    |  |  |  |-- class: 0
    |  |  |-- NUM >  0.5
    |  |  |  |-- class: 2
    |  |-- n_tokens_2 >  1.1
    |  |  |-- NUM <= 0.5
    |  |  |  |-- class: 0
    |  |  |-- NUM >  0.5
    |  |  |  |-- class: 1

## Decision tree for Tagger T~pos~

    |-- NUM <= 0.5
    |  |-- ADJ <= 1.4
    |  |  |-- n_tokens_2 <= 1.1
    |  |  |  |-- token_index_1 <= 0.4
    |  |  |  |  |-- token_index_2 <= 0.6
    |  |  |  |  |  |-- n_tokens_3 <= -0.0
    |  |  |  |  |  |  |-- class: 0
    |  |  |  |  |  |-- n_tokens_3 >  -0.0
    |  |  |  |  |  |  |-- class: 0
    |  |  |  |  |-- token_index_2 >  0.6
    |  |  |  |  |  |-- ADP <= 2.3
    |  |  |  |  |  |  |-- class: 0
    |  |  |  |  |  |-- ADP >  2.3
    |  |  |  |  |  |  |-- class: 0
    |  |  |  |-- token_index_1 >  0.4
    |  |  |  |  |-- ADP <= 2.3
    |  |  |  |  |  |-- CCONJ <= 6.0
    |  |  |  |  |  |  |-- class: 0
    |  |  |  |  |  |-- CCONJ >  6.0
    |  |  |  |  |  |  |-- class: 0
    |  |  |  |  |-- ADP >  2.3
    |  |  |  |  |  |-- n_tokens_3 <= -0.0
    |  |  |  |  |  |  |-- class: 0
    |  |  |  |  |  |-- n_tokens_3 >  -0.0
    |  |  |  |  |  |  |-- class: 0
    |  |  |-- n_tokens_2 >  1.1
    |  |  |  |-- class: 0
    |  |-- ADJ >  1.4
    |  |  |-- token_index_2 <= 0.6
    |  |  |  |-- token_index_1 <= 0.4
    |  |  |  |  |-- class: 0
    |  |  |  |-- token_index_1 >  0.4
    |  |  |  |  |-- n_tokens_2 <= 1.1
    |  |  |  |  |  |-- n_tokens_3 <= -0.0
    |  |  |  |  |  |  |-- class: 0
    |  |  |  |  |  |-- n_tokens_3 >  -0.0
    |  |  |  |  |  |  |-- class: 0
    |  |  |  |  |-- n_tokens_2 >  1.1
    |  |  |  |  |  |-- class: 0
    |  |  |-- token_index_2 >  0.6
    |  |  |  |-- n_tokens_3 <= -0.0
    |  |  |  |  |-- class: 0
    |  |  |  |-- n_tokens_3 >  -0.0
    |  |  |  |  |-- class: 0
    |-- NUM >  0.5
    |  |-- n_tokens_2 <= 1.1
    |  |  |-- n_tokens_3 <= -0.0
    |  |  |  |-- token_index_0 <= 0.3
    |  |  |  |  |-- token_index_2 <= 0.6
    |  |  |  |  |  |-- token_index_1 <= 0.4
    |  |  |  |  |  |  |-- class: 2
    |  |  |  |  |  |-- token_index_1 >  0.4
    |  |  |  |  |  |  |-- class: 2
    |  |  |  |  |-- token_index_2 >  0.6
    |  |  |  |  |  |-- class: 2
    |  |  |  |-- token_index_0 >  0.3
    |  |  |  |  |-- class: 2
    |  |  |-- n_tokens_3 >  -0.0
    |  |  |  |-- token_index_1 <= 0.4
    |  |  |  |  |-- class: 2
    |  |  |  |-- token_index_1 >  0.4
    |  |  |  |  |-- class: 0
    |  |-- n_tokens_2 >  1.1
    |  |  |-- class: 1

# Annotation {#app:ann}

This Section describes the annotated subset of recipes, the rationale
for this selection, before showing the results in terms of linking. The
selection is made with the objective to be as representative as possible
for the main dishes in the test set. So ideally we would sample
ingredients from the main dish recipes in such a way that the
distribution over the predicted classification codes is even. The main
complication is that we have resources to annotate 10 recipes which
topically contain about 140 ingredients in total, but the classification
model can give about 4,000 different FoodEx2-codes as output. To be as
representative as possible we selected a subset of recipes of which the
ingredients have a modal value of unique predicted classification codes.
Figure [7](#fig:hist_unique){reference-type="ref"
reference="fig:hist_unique"} shows the distribution of number of unique
prediction classes $\hat{C}$ over 1,000 random subsets of 10 main course
recipes, and highlights the bin from which we randomly selected the
subset for annotation.

<figure id="fig:hist_unique">

<figcaption>Distribution of number of unique prediction classes over
1,000 subsets of 10 main course recipes</figcaption>
</figure>


[^6]: <https://github.com/robzwitserlood/NL-recipe-sustainability>

[^7]: <https://www.24kitchen.nl>

[^8]: Translation via Google Cloud Translation API
    <https://cloud.google.com/translate>

[^9]: EFSA FoodEx2 backend
    <https://github.com/openefsa/foodex2-sca-backend>

[^10]: https://nevo-online.rivm.nl/

[^6]: <https://scikit-learn.org>

[^1]: <https://www.allrecipes.com>

[^2]: <https://www.nltk.org>

[^3]: <https://stanfordnlp.github.io/CoreNLP/>

[^4]: <https://doi.org/10.17026/dans-xvh-x9wz>

[^5]: <https://www.english-corpora.org/hansard>
