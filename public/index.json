
[{"content":"","date":"1 January 2021","externalUrl":null,"permalink":"/","section":"","summary":"","title":"","type":"page"},{"content":" Introduction # Food systems have a substantial global environmental footprint by contributing to problems such as climate change, biodiversity loss, and land system change [@Willett2019]. In fact, the food supply chain accounts for one quarter of global anthropogenic Green House Gas (GHG) emission, one third of global terrestrial acidification and about 80% of eutrophication. Furthermore, at least 6 out of 17 of the United Nations' Sustainable Development Goals are concerned with food [@Poore2018a; @Johnston2016].\nThe food supply chain can be segmented into seven stages: 1) Land use change, 2) Farming, 3) Animal food production, 4) Processing, 5) Transport, 6) Retail and 7) Packaging [@Poore2018a]. GHG emission from land use change and farming account for more than 80% of the footprint for most foods [@Poore2018a]. The amount and kind of food that a person habitually consumes is called a diet. Food products in a diet largely determines its GHG footprint [@Sandstrom2018]; dairy, meat and eggs account for 80% and plant based food for 20%. Although, the country of origin of the food products also influences dietary emissions, mainly because of differences in land use change [@Sandstrom2018].\nFood products in overall diets have been studied in various ways such as recipe analysis, food diaries, direct observations of food consumption or surveys. The benefits of online recipe analysis over other methods are: more reactive than food diaries, more scalable than direct observations of food consumption, and less biased than surveys [@FCRN2015; @Mouritsen2017]. An example of online recipe based work is an analysis of the change towards meat-free diets in Germany [@Asano2019].\nA number of complications have been identified in computational assessment of recipes for sustainability. First of all, Natural Language Processing (NLP) is technologically complex once recipes do not come from the same source [@VanErp2021]. Examples of NLP-issues are: Part-Of-Speech tagging is not reliable for sentences in the food recipe domain due to their imperative nature [@Goncalo2018; @Chang2018], interpreting sentences in the food recipe task which are either poorly structured by the author or contain a misleading expression [@Goncalo2018], quantities can be expressed by numerals, expressed by fractions or spelled out and units can be expressed as metric, imperial, or other measurements such as teacups [@VanErp2018]. Another complication addressed in [@VanErp2021] is that there are linguistic, conceptual and terminological gaps between recipe and GHG emission Knowledge Bases (KBs), that is, KBs that allocate a quantity of GHG emission to a food product [@VanErp2021]. Issues in bridging the gap between recipes and GHG emission KBs are: GHG emission KBs can be structured according to a variety of food data systems developed by different countries in different languages (such as FoodEx2, LanguaL, GloboDiet and NEVO) [@Brown2017; @Ireland2000], and computational food matching according to the definitions of a food data system is challenging when food is described differently than in the food data system [@KorousicSeljak2018]. To the best of our knowledge, no research has been published on how to resolve the complications mentioned in this introduction for online recipes written in the Dutch language. Therefore our main contribution to existing literature is that we focus our efforts on Dutch recipes only. As a result, our research question is: To what extent can recipe footprint assessment be automated by linking ingredients, quantities and units from Dutch recipe text to a GHG emission KB? The main question is broken down into four sub-questions:\nTo what extent can descriptions of ingredients, quantities and units be recognized in Dutch recipe text?\nTo what extent can recognized descriptions of ingredients be linked to a GHG emission KB?\nTo what extent can recognized descriptions of quantities and units be linked to a GHG emission KB?\nTo what extent can a recipe\u0026rsquo;s footprint be assessed based on a linked GHG emission KB?\nThe remainder of this paper is structured as follows. Related work is described in Section \\[sec:rel\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;sec:rel\u0026rdquo;}. Our methodology is described in Section \\[sec:meth\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;sec:meth\u0026rdquo;}. The results are shown in Section \\[sec:res\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;sec:res\u0026rdquo;}. In Section \\[sec:eva\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;sec:eva\u0026rdquo;}, we discuss the limitations and implications of our results. The conclusions and future work are described in Section \\[sec:conc\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;sec:conc\u0026rdquo;}.\nRelated Work # We describe related work on recipe data, description recognition, sustainable food data and ingredient linking in the following subsections.\nRecipe data # Recipe data is available from various sources and in different formats and languages [@VanErp2021]. We clustered publications of recipe data along the two dimensions most relevant to this study: 1) Language (Non-Dutch vs Dutch) and 2) ingredient annotation (with vs without a golden standard).\nThe first cluster consists of Non-Dutch data sets without golden standard. The Cooking Recipes Without Border data set (CRWB) [@Frederic2018; @Frederic2018a] is an initiative to collaborate on a linked open data collection of cooking recipes that follows the FAIR model [@Wilkinson2016]. CRWB contains observations of ingredients in dishes by users which can be used as annotation labels, but strictly speaking are not ground truth labels. In addition, CRWB has cooking actions described in French. The Recipe1M+ dataset consists of 1M recipes with images of corresponding dishes scraped from non-Dutch cooking websites [@Marin2021]. The Recipe1M+ data set contains ingredients, units and quantities annotations which were are automatically tagged by a \u0026lsquo;quantity-unit-ingredient\u0026rsquo; sequence pattern.\nSecondly, the cluster of related work that consists of Non-Dutch data sets with a golden standard. RecipeDB is a structure compilation of over 100k recipes in which ingredients are labelled and linked to nutritional values [@Batra2020]. The New York Times published a dataset that contains 180K ingredient lines in English language in which are manually annotated for ingredients, units and amounts [@Greene2015]. FoodBase [@Popovski2019] is data set with recipes in English language extracted from Allrecipes 1 which are annotated with a rule-based food Named Entity Recognition (NER) method. For a subset of 1,000 recipes, the annotation was manually evaluated to create a golden standard. RecipeNLG [@BienMichaandGilski2020] is a dataset of over 2M English language cooking recipes for the purpose of generating recipes, it was built on top of Recipe1M+ [@Marin2021]. A subset of 500 recipes in RecipeNLG was manually annotated for food entities to train a NER model that was used extract ingredients from the rest of the data set.\nThe third cluster consists of Dutch data sets with a golden standard. A data set with 27K Dutch recipes from newspaper articles is published by [@VanErp2018]. A subset of 100 recipes are manually annotated by tagging ingredients, quantities and units in the corpus of the articles. The final cluster consists of Dutch data sets without a golden standard. In addition to a data set with recipes from news papers, [@VanErp2018] also made a data set available that contains 16K recipes scraped from the website of one of the largest Dutch supermarket chains. The recipes are marked up with schema.org information such as ingredients, units and quantities.\nDescription recognition # Description recognition is a task that can be interpreted as sequence tagging, that is, the tokens in a food text need to be tagged as ingredient, unit or quantity. Sequence tagging is a structured learning problem in which a class or label is assigned to each token in an input sequence [@Tang2010]. We clustered previous work into lexicon-based, rule-based and learning-based sequence tagging approaches. To start with lexicon-based sequence tagging, [@Mazzei] compared to what extent 4 Italian computational lexicons cover the words in a recipe for extraction of the recipe\u0026rsquo;s nutritional values. A lexicon-based method to lift ingredients is presented in [@VanErp2018], followed by Named Entity Linking (NEL) based on string match or DBpedia spotlight [@Daiber2013]. Secondly, we discuss examples of rule-based approaches. Regular expressions are used by [@VanErp2018] on tokens for NER of quantities and units. In the approach of [@Haussmann2019], first parts of the ingredient sentence that are assumed not or less relevant (e.g. parenthesized statements) are stripped, than numerical tokens are selected as quantities and the NLTK toolkit 2 is used to extract the ingredient from the remaining part of the sentence. A rule-based food NER called FoodIE is introduced by [@Popovski2019a], it is based on the UCREL Semantic Analysis System (USAS) tagger presented in [@Rayson2004]. Lastly we describe the learning-based approaches. Ingredient sequences are tagged with conditional random fields in [@Greene2015]. Stanford CoreNLP 3 Part-of-Speech tagging and human annotation is used in [@Chang2018] to train a model that parses ingredients and actions. A deep learning NER tool are trained in [@Yamakata2020; @BienMichaandGilski2020] to extract a number of recipe named entities. A CNN-based sequence encoder based on [@Zhang2015] is used in [@Yamaguchi2020] to detect non-ingredient such as kitchenware in recipe ingredient lists. A Stanford NER Tagger is trained and utilized by [@Diwan2020] to annotate the ingredients for clustering purposes.\nSustainable food data # Sustainability data and knowledge sources for food are incoherent and not consistently available [@VanErp2021]. Food consumption data of the Food and Agricultural Organization (FAO) contains useful indicators of the environmental impact of ingredients [@Quadros2019]. The SHARP Indicators Database (SHARP-ID4) is a European-wide applicable public database for indicators of environmental sustainability of primary food products [@Mertens2019]. A construction process and maintenance plan for a unified food knowledge graph is published by [@Haussmann2019]. First results of the BONSAI project, which aims for an open source dataset and ontology for food product life cycle assessment, are presented by [@Ghose2019]. A carbon footprint study is conducted by [@Toledo2020] based on a dummy database since the required knowledge bases were not available at that point of time. While sustainability data might be available for individual ingredients, it is still rare for entire recipes [@Speck2020; @Roos2013]. A methodology to assess the environmental impact of a whole recipe rather then the individual ingredients in presented in [@Toledo2020].\nIngredient linking # Various food description, classification and coding systems have been developed by different countries and organisations [@Brown2017; @Ireland2000]. Therefore, the International Network of Food Data Systems (FAO/INFOODS) prepared guidelines for food matching, that is, the process of linking food consumption data to food databases [@Barbara2012]. An example of a coding system is called FoodEx2, which stands for \u0026lsquo;Food classification and description for Exposure assessment\u0026rsquo; [@Food2014]. The system has a hierarchical structure of parent-child relationships between food groups, categories and items. All terms have a unique alphanumerical code, a name and description in English language. Ingredients encoding at the sentence level is presented in [@Zan2020]. The ingredient lines in the recipe list are encoded with BERT to obtain an ingredient embedding, which is concatenated with the title and instruction embeddings to a recipe embedding vector. FoodIE [@Popovski2019a] is used in [@Popovski2019] to link food entities to semantic tags in the \u0026lsquo;Food and drink\u0026rsquo; category of the Hansard corpus 5. NEL based on string match or DBpedia spotlight [@Daiber2013] is presented in [@VanErp2018].\nAnalysis of related work # Regarding recipe data, our analysis shows that completely manually annotated data sets are rare ([@Greene2015] only). Manual annotation of a subset is a more common approach, either for training or evaluation of ingredient extraction models. Regarding description recognition, lexicon-based, rule-based and learning-based sequence tagging approaches have been published. Comparing the performance of the three approaches is complex since different data sets, definitions of entities, and ground truth labels are used. In addition, we note that the rule-based approach of FoodIE [@Popovski2019a] is insightful due to transparency in used features. Regarding sustainability food data and linking, string matching and classification approaches to linking have been published. In addition, standardization of food coding systems is an ongoing process.\nMethodology # ::: figure* :::\nWe take an empirical approach to the the task of linking online Dutch recipes to a sustainability KB. We begin by formalizing the problem, we then describe the proposed model and experimental design.\nProblem formulation # Our starting point is a conceptualization of a recipe as an object denoted by $R$. A recipe object $R$ has two attributes. The first attribute is a set $D$ that contains descriptions of ingredients and/or utensils required to produce the dish. The second attribute of recipe $R$ is a footprint $F$ which we define as the distribution of GHG emission per unit mass allocated to the dish. More specifically, footprint $F$ is defined as the 5^th^ percentile $F^{5th}$, mean $F^{mean}$ and 95^th^ percentile $F^{95th}$ of the distribution of GHG emission per unit mass. Other attributes of a recipe, such as the preparation steps, are not considered since they are not in the scope of this work. Based on the formulation described in this section, the problem addressed in our research question is formalized as: Given a set of descriptions $D$ of recipe $R$, can we make a conservative footprint prediction $\\hat{F}$ for the actual footprint $F$ of recipe $R$? We consider a footprint prediction conservative if $\\hat{F}^{5th} \\le F^{5th}$ and $\\hat{F}^{95th} \\ge F^{95th}$.\nModel # Our model is structured according to the sub-questions of our research question, therefore we define four tasks in our model: 1) tagging, 2) classification, 3) conversion and 4) emission allocation. Besides these four tasks, we define an quantified ingredient object denoted by $I_q$. A quantified ingredient object $I_q$ is a conceptualization of an ingredient or utensil required to produce the dish. We assume that each description $d$ in the set of descriptions $D$ is an attribute of a quantified ingredient object $I_q$. Besides a description $d$, a quantified ingredient object $I_q$ has three other attributes: 1) a food class denoted by $C$, 2) a mass denoted by $M$ and 3) a distribution of GHG emission per unit mass denoted by $E$. The objective of the four defined tasks (tagging, classification, conversion and emission allocation) is to make a prediction of the attributes of a quantified ingredient object $I_q$ based its description attribute $d$. Our modeling decisions per task are explained in more detail in the following paragraphs.\nFor the tagging task, we assume that each item $d$ in the set of descriptions $D$ is a sequence of tokens. Each token in each description $d$ represent either the food class $C$ or the mass $M$ of its quantified ingredient object $I_q$. If a token represents mass $M$, it can either have label $U$ which stands for unit measure or $Q$ which stands for quantity. If a token represents the food class $C$, it has label $I$ which stands for ingredient. In our model, each sequence of tokens $d$ is enriched to a sequence of labeled tokens, which we denote by $d^{tag}$. The objective of the tagging task is to predict the labels in $d^{tag}$. The predicted sequence of labeled tokens is denoted by $\\hat{d}^{tag}$, a set of sequences of labeled tokens is denoted by $\\hat{D}^{tag}$.\nThe next task in our model is classification. We assume that a prediction of the food class can be made based on the token(s) labelled as ingredient $I$. We denote the prediction of the food class as $\\hat{C}$. It is important to note that the tokens used in the classification task depend on the output of the tagging task.\nThe next task in our model is conversion. We assume that a prediction of the mass can be made based on the token(s) labeled as quantity $Q$ or unit $U$ and based on the predicted food class $\\hat{C}$. We denote the prediction of the mass as $\\hat{M}$. It is important to note that the output of the conversion task depend on both the output of the tagging and classification task.\nThe final task in our model is emission allocation. The first assumption is that a prediction of the GHG emission can be made based on the predicted food class $\\hat{C}$ and predicted mass $\\hat{M}$. We denote the prediction of the GHG emission as $\\hat{E}$. The second assumption is that a prediction of the footprint of a recipe can be made based on the mass weighted average of the predicted GHG emission $\\hat{E}$ for all quantified ingredients represented by description $d$ in set of descriptions $D$. We denote the prediction of footprint as $\\hat{F}$.\nExperimental design # Our experimental design operationalizes our model in the form of a data system. Figure \\[fig:exp_Setup\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;fig:exp_Setup\u0026rdquo;} show a high level graphical representation of the data system in which we define three independent variables and four evaluation metrics (or dependent variables). Inputs and output are connected via a number of data processing steps that we define as the pipeline. Since the pipeline contains trainable models, we define two cycles: 1) the train \u0026amp; validation cycle and 2) the test cycle. The code of this data system can be found on Github1.\nIndependent variables # The three independent variables and the values they can take are as follows. The first variable is tagging model $T$. It represents which tagging model is used in the configuration of the pipeline. The three possible values for $T$ are $T_{sti}$, $T_{num}$ and $T_{pos}$. The next variable is food classification set $L_C$. It represents the unique labels that can be assigned to the food class $C$ of a quantified ingredient objects $I_q$ in the configuration of the pipeline. The five possible values for $L_C$ are $E$, $C$, $P$, $M$, $H$ each of which represent a set of classification labels from the FoodEx2 system with a maximum granularity level. The third variable is food classification threshold $P_t$. It represents a threshold to a softmax value. The prediction class $\\hat{C}$ is based on the output class $\\hat{C}{max}$ with the highest softmax value $P{max}$. If $P_{max} \u0026lt; P_t$, than $\\hat{C}$ is set to $NFOOD$. If $P_{max} \\ge P_t$, than $\\hat{C}$ is selected by mapping $\\hat{C}_{max}$ to one the classes in $L_C$. The nine possible values for $P_t$ are $0.1$, $0.2$, $0.25$, $0.3$, $0.4$, $0.45$, $0.5$, $0.6$ and $0.7$.\nPipeline # The input of the pipeline is a set of recipe data, which was scraped from the website of 24Kitchen2 in April 2021. It consists of 7,097 recipes which have 82,263 ingredient/utensil descriptions which in total comprise of 240,131 tokens. All tokens are annotated in the HTML-code as \u0026lsquo;quantity\u0026rsquo;, \u0026lsquo;unit\u0026rsquo; or \u0026lsquo;ingredient\u0026rsquo;, except for 56 tokens which we choose to label as \u0026lsquo;others\u0026rsquo;. A summary of the exploratory data analysis is provided in Appendix \\[app:eda\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;app:eda\u0026rdquo;}.\nThe preprocess component of the pipeline takes in the recipe data and processes it in three steps: first tokenization, second Part-Of-Speech (POS) tagging and third cleaning (removal of punctuation and spaces).\nThe tagging component takes in the tokens of a description $d$, engineers and scales the features that form the input of the tagging model $T$. The output of the tagging model is a predicted tag label for each token.\nThe classification component selects the tokens tagged as ingredient $I$, translates them to English language3 and feeds them to a classification model that is pretrained by the European Food Safety Agency (EFSA) 4. The prediction class $\\hat{C}$ is based on the output class $\\hat{C}{max}$ with the highest softmax value $P{max}$. If $P_{max} \u0026lt; P_t$, than $\\hat{C}$ is set to $NFOOD$. If $P_{max} \\ge P_t$, than $\\hat{C}$ is selected by mapping $\\hat{C}_{max}$ to one the classes in $L_f$. The mapping is based on the hierarchy of the FoodEx2 coding system. The five levels of granularity are from low to high: 1) Hierarchy Term (H), 2) Aggregation Term (M), 3) Non-specific Term (P), 4) Core Term (C) and 5) Extended Term (E).\nThe conversion component selects the tokens tagged as quantity $Q$ or unit $U$. Tokens tagged as quantity $Q$ are converted to a numerical value if possible. Tokens tagged as unit are looked-up in three tables. The first table is dictionary with common units of mass, that is, \u0026lsquo;g\u0026rsquo;, \u0026lsquo;kg\u0026rsquo;, \u0026lsquo;gr\u0026rsquo;, \u0026lsquo;gram\u0026rsquo; and \u0026lsquo;kilo\u0026rsquo;. The second table is a dictionary with common units of volume, that is, \u0026lsquo;ml\u0026rsquo;, \u0026rsquo;l\u0026rsquo; and \u0026rsquo;liter\u0026rsquo;. The third table is an extensive database called Portie-online [@RIVM2020] with units of measure used for food and their respective mass in grams for a food category specified in the NEVO5 coding system. The NEVO code is obtained by mapping the predicted class $\\hat{C}$, which is expressed in FoodEx2, to a NEVO code. If no tokens with tag \u0026lsquo;unit\u0026rsquo; are present (e.g. \u0026lsquo;2 onions\u0026rsquo;), than we assume the unit of measure is not explicitly mentioned but does exist. In that case, we lookup the Dutch word \u0026lsquo;stuks\u0026rsquo; (in English \u0026rsquo;number of items\u0026rsquo;) in the Portie-online database. If a numerical value is found for both the quantity and unit, than $\\hat{M}$ is set to the product of these values. If at least one of them could not be converted to a numerical value, than $\\hat{M}$ is set to zero.\nThe emission component takes in the predicted class $\\hat{C}$ and mass $\\hat{M}$. The GHG emission $\\hat{E}$ is selected based on a lookup of the predicted class $\\hat{C}$ in the GHG knowledge base. If the prediction class $\\hat{C}=NFOOD$, we assume the most conservative values in the knowledge base for the bounds, that is, $\\hat{E}^{5th}=min(E^{5th})$ and $\\hat{E}^{95th}=max(E^{95th})$, and $\\hat{E}^{mean}=mean(E^{mean})$.\nThe evaltag component takes in the actual tagged descriptions $D^{tag}$ and predicted tagged descriptions $\\hat{D}^{tag}$. The $F_1^{tag}$ value is calculated based on the macro averaged precision $Pr$ and macro averaged recall $Re$ per classed as shown in Equations \\[eq:tag_pr\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;eq:tag_pr\u0026rdquo;}, \\[eq:tag_re\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;eq:tag_re\u0026rdquo;} and \\[eq:tag_k1\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;eq:tag_k1\u0026rdquo;}.\n$$\\label{eq:tag} \\begin{align} Pr = \\frac{1}{3} \\sum_{l \\in \\{Q,U,I\\}} Pr_l = \u0026 \\frac{1}{3} \\sum_{l \\in \\{Q,U,I\\}} \\frac{TP_l}{TP_l+FP_l} \\label{eq:tag_pr} \\\\ Re = \\frac{1}{3} \\sum_{l \\in \\{Q,U,I\\}} Re_l = \u0026 \\frac{1}{3} \\sum_{l \\in \\{Q,U,I\\}} \\frac{TP_l}{TP_l+FN_l} \\label{eq:tag_re} \\\\ F_1^{tag} = \u0026 \\frac{2 Pr \\cdot Re}{Pr + Re} \\label{eq:tag_k1} \\end{align}$$$TP_l$, $FN_l$ and $FP_l$ respectively denote the number of True Positive, False Negative and False Positive tag predictions of tokens with label $l$.\nThe evallink component takes in predicted class $\\hat{C}$, predicted mass $\\hat{M}$, set of classification labels $L_C$, and ground truth labels $C$ and mass $M$. Since annotation has to be done manually and our resources our limited, a subset of recipes in the test set is selected for annotation. More details about the annotated subset of recipes and the rationale for this selection is detailed in Appendix \\[app:ann\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;app:ann\u0026rdquo;}. The classification accuracy $a_C$ and mass ratio $r_M$ are calculated as described in Equation \\[eq:link\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;eq:link\u0026rdquo;}.\n$$\\label{eq:link} \\begin{align} a_C = \u0026\\frac{TP}{TP+FP+TN+FN} \\label{eq:link_acc} \\\\ r_M = \u0026\\frac{\\hat{M}}{M} \\label{eq:link_mr} \\end{align}$$ Hypothesis # Our experiment is designed to be able to test the following hypothesis.\n::: hypothesis []{#hy:recall label=\u0026ldquo;hy:recall\u0026rdquo;} Given a food classification model, a set of food classes $L_C$ and a classification threshold $P_t$, ...\n... a higher ingredient recall $Re_I$ leads to higher classification accuracy $a_C$\n... a higher quantity recall $Re_Q$ or unit recall $Re_U$ leads to higher mass ratio $r_M$ :::\n::: hypothesis []{#hy:granularity label=\u0026ldquo;hy:granularity\u0026rdquo;} Given a food classification model, a tagger model $T$ and a classification threshold $P_t$, ...\n... a more granular set of food classes $L_C$ leads to lower predictions accuracy $a_C$\n... a more granular set of food classes $L_C$ leads to higher mass ratio $r_M$ :::\n::: hypothesis []{#hy:threshold label=\u0026ldquo;hy:threshold\u0026rdquo;} Given a food classification model, a set of food classes $L_C$ and a tagger model $T$, ...\n... a higher classification threshold $P_t$ leads to a higher classification accuracy $a_C$\n... a higher classification threshold $P_t$ leads to a lower mass ratio $r_M$ :::\nTrained models # The train and validation cycle is setup to design the three taggers represented by $T_{sti}$, $T_{num}$ and $T_{pos}$. The data is split in three sets: train, validation and test (see Table 1{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;tabel:splitsizes\u0026rdquo;}). The split is made by stratified random sampling on a recipe level based on recipe category (Main, Appetizer, Dessert and Unknown). Stratification on recipe category is done to prevent a bias in the model towards a specific category. Splitting on a recipe level is done to make sure that each split has a representative set of ingredients.\n::: {#tabel:splitsizes} Recipes Ingredients Tokens\nTrain 5,640 (79.5%) 65,599 (79.7%) 191,965(79.8%) Validation 726 (10.2%) 8,226 (10.0%) 24,131 (10.0%) Test 731 (10.3%) 8,438 (10.3%) 24,475 (10.2%) Total 7,097 82,263 240,571\n: Size of data splits: Absolute size (relative size as percentage of total) :::\nAll tagging models are rule-based models that are implemented by fitting a decision tree. We used the Scikit-learn6 package in Python, which uses the CART algorithm [@Dension1998] to fit trees. Feature engineering for the decision trees is done in a three-step process. In the first step, features are based on the exploratory data analysis (see Appendix \\[app:eda\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;app:eda\u0026rdquo;}). We create six binary features for $T_{sti}$: three features that respectively represent description length of $1$, $2$ or $3$ tokens and three features that respectively represent token position of $0$, $1$ or $2$. In the second step, an additional feature (on top of the features for $T_{sti}$) is selected based on the hypothesis that tokens that are numerical most probably represent a quantity. Therefore tagger $T_{num}$ is trained based on the six features of $T_{sti}$ and the binary feature that describes if a token is numerical. In the third step, the tokens that are incorrectly classified by $T_{num}$ are analysed by Part-Of-Speech (POS) tags. The POS tags in the top 5 incorrectly classified tokens are selected as potential additional features (shown in subsection \\[fig:RB_error_1\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;fig:RB_error_1\u0026rdquo;}). All combinations of these potential additional features are tested, which resulted in three additional binary features for $T_{pos}$ that lead to a performance increase compared to $T_{num}$: the features describe if a token has \u0026lsquo;adj\u0026rsquo;, \u0026lsquo;adp\u0026rsquo; or \u0026lsquo;cconj\u0026rsquo; POS tag.\nHyperparameter tuning is done as follows. The maximum tree depth and initial random state of the decision tree models are selected as hyperparameter to tune. A grid search is done for $T_{sti}$, $T_{num}$ and $T_{pos}$ in which all combinations of maximum tree depth of $1$, $2$, ..., $10$ and $None$ and initial random state of $1$, $2$, ..., $20$. Within the set of models with the highest $F_1^{tag}$-score, the model with the lowest maximum tree depth is selected as final configuration.\n\\section{Results} \\label{sec:res} This section describes and interprets the data collected during the experiment described in Section \\ref{sec:meth}. To give a first impression of the output of the pipeline, we present Example \\ref{ex:wijnazijn} that is correctly tagged, classified and converted and Figure \\ref{fig:main_metrics} that gives an overview of the evaluation metrics for tagging, classification and conversion.\n\\begin{example}\\label{ex:wijnazijn} The tokens in description $d$ 20 ml rode wijnazijn' (in English 20 ml red wine vinegar\u0026rsquo;) are respectively tagged as quantity, unit, ingredient, ingredient. The quantity and unit are converted to 20 grams and the ingredient is classified as FoodEx2 code A044M' that represents Vinegar, wine\u0026rsquo;. \\end{example}\n\\begin{figure}[H] %\\includegraphics[width=6cm]{example-image-duck} \\input{Images/pgf/main_metrics.pgf} \\caption{Overview of tagging and linking evaluation metrics for annotated recipes in test set, maximum per tagger} \\label{fig:main_metrics} \\end{figure}\nOur two main observations for Figure \\ref{fig:main_metrics} are: 1) the $F_1^{tag}$-score increases from $T_{sti}$ to $T_{num}$ and from $T_{num}$ to $T_{pos}$ (which is further analyzed in Subsection \\ref{subsec:TVcycle}), and 2) the maximum classification accuracy $a_C$ and mass ratio $r_M$ do not change from $T_{sti}$ to $T_{num}$ and increase from $T_{num}$ to $T_{pos}$ (which is further analyzed in Subsection \\ref{subsec:TESTcycle} and \\ref{subsec:TESTcycle} respectively).\n\\subsection{Tagging} \\label{subsec:TVcycle} The confusion matrix of $T_{sti}$ over the validation set is shown in Table \\ref{tabel:RB_per_1}. Our main observation from this table is that there are no quantity-tokens tagged as unit and vise versa.\n\\begin{table}[H] \\begin{tabular}{ll|rrrr|rr} \\toprule \u0026amp; \u0026amp; \\multicolumn{6}{l}{Prediction label} \\ \u0026amp; \u0026amp; qty \u0026amp; ingr. \u0026amp; others \u0026amp; unit \u0026amp; sum \u0026amp; $Re_l$ \\ \\midrule \\multirow{6}{}{\\rotatebox[origin=c]{90}{Actual label}} \u0026amp; qty \u0026amp; 6,893 \u0026amp; 14 \u0026amp; 0 \u0026amp; 0 \u0026amp; 6,907 \u0026amp; 99.8% \\ \u0026amp; ingr. \u0026amp; 482 \u0026amp; 10,772 \u0026amp; 0 \u0026amp; 901 \u0026amp; 12,155 \u0026amp; 88.6% \\ \u0026amp; oth. \u0026amp; 46 \u0026amp; 6 \u0026amp; 0 \u0026amp; 4 \u0026amp; 56 \u0026amp; 0% \\ \u0026amp; unit \u0026amp; 0 \u0026amp; 22 \u0026amp; 0 \u0026amp; 4,991 \u0026amp; 5,013 \u0026amp; 99.6% \\ \\cmidrule{2-8} \u0026amp; sum \u0026amp; 7,421 \u0026amp; 10,814 \u0026amp; 0 \u0026amp; 5,896 \u0026amp; 24,131 \u0026amp; 96.0%\\textsuperscript{} \\ \u0026amp; $Pr_l$ \u0026amp; 92.9% \u0026amp; 99.6% \u0026amp; 0% \u0026amp; 84.7% \u0026amp; 92.4%\\textsuperscript{*} \u0026amp; 94.2%\\textsuperscript{**} \\ \\bottomrule \\end{tabular} \\caption{Confusion matrix over validation set for $T_{sti}$. *) is macro averaged recall $Re$ / precision $Pr$, **) is $F_1^{tag}$-score} \\label{tabel:RB_per_1} \\end{table}\nTable \\ref{tabel:RB_per_2} shows the confusion matrix of $T_{num}$ over the validation set. Our first observation from this table is that the additional `like-num\u0026rsquo; feature lead to an increase of $F_1^{tag}$-score to 96% (increase of 2% compared to $T_{sti}$ shown in Table \\ref{tabel:RB_per_1}). The second observation is that the number of ingredient-tokens tagged as quantity is $6$, a reduction of $476$ compared to Table \\ref{tabel:RB_per_1} which is illustrated by Example \\ref{ex:olijfolie1}.\n\\begin{table}[H] \\begin{tabular}{ll|rrrr|rr} \\toprule \u0026amp; \u0026amp; \\multicolumn{6}{l}{Prediction label} \\ \u0026amp; \u0026amp; qty \u0026amp; ingr. \u0026amp; others \u0026amp; unit \u0026amp; sum \u0026amp; $Re_l$ \\ \\midrule \\multirow{6}{}{\\rotatebox[origin=c]{90}{Actual label}} \u0026amp; qty \u0026amp; 6,893 \u0026amp; 14 \u0026amp; 0 \u0026amp; 0 \u0026amp; 6,907 \u0026amp; 99.8% \\ \u0026amp; ingr. \u0026amp; 6 \u0026amp; 11,248 \u0026amp; 0 \u0026amp; 901 \u0026amp; 12,155 \u0026amp; 92.5% \\ \u0026amp; oth. \u0026amp; 23 \u0026amp; 29 \u0026amp; 0 \u0026amp; 4 \u0026amp; 56 \u0026amp; 0% \\ \u0026amp; unit \u0026amp; 0 \u0026amp; 22 \u0026amp; 0 \u0026amp; 4,991 \u0026amp; 5,013 \u0026amp; 99.6% \\ \\cmidrule{2-8} \u0026amp; sum \u0026amp; 6,922 \u0026amp; 11,313 \u0026amp; 0 \u0026amp; 5,896 \u0026amp; 24,131 \u0026amp; 97.3%\\textsuperscript{} \\ \u0026amp; $Pr_l$ \u0026amp; 99.6% \u0026amp; 99.4% \u0026amp; 0% \u0026amp; 84.7% \u0026amp; 94.6%\\textsuperscript{*} \u0026amp; 95.9%\\textsuperscript{**} \\ \\bottomrule \\end{tabular} \\caption{Confusion matrix over validation set for $T_{num}$. *) is macro averaged recall $Re$ / precision $Pr$, **) is $F_1^{tag}$-score} \\label{tabel:RB_per_2} \\end{table}\nTable \\ref{tabel:RB_per_3} shows the confusion matrix of $T_{pos}$ over the validation set. Our first observation from this table is that the additional POS features lead to an increase of $F_1^{tag}$-score to 98% (increase of 2% compared to $T_{num}$ in Table \\ref{tabel:RB_per_2}). Our second observations from Table \\ref{tabel:RB_per_3} is that the number of ingredient-tokens tagged as unit is $239$, a reduction $662$ compared to Table \\ref{tabel:RB_per_2} which is illustrated by Example \\ref{ex:olijfolie1}. Thirdly, the number of unit-tokens tagged as ingredient is $209$, an increase of $187$ compared to Table \\ref{tabel:RB_per_2}.\n\\begin{table}[H] \\begin{tabular}{ll|rrrr|rr} \\toprule \u0026amp; \u0026amp; \\multicolumn{6}{l}{Prediction label} \\ \u0026amp; \u0026amp; qty \u0026amp; ingr. \u0026amp; others \u0026amp; unit \u0026amp; sum \u0026amp; $Re_l$ \\ \\midrule \\multirow{6}{}{\\rotatebox[origin=c]{90}{Actual label}} \u0026amp; qty \u0026amp; 6,893 \u0026amp; 14 \u0026amp; 0 \u0026amp; 0 \u0026amp; 6,907 \u0026amp; 99.8% \\ \u0026amp; ingr. \u0026amp; 6 \u0026amp; 11,910 \u0026amp; 0 \u0026amp; 239 \u0026amp; 12,155 \u0026amp; 98.0% \\ \u0026amp; oth. \u0026amp; 23 \u0026amp; 30 \u0026amp; 0 \u0026amp; 3 \u0026amp; 56 \u0026amp; 0% \\ \u0026amp; unit \u0026amp; 0 \u0026amp; 209 \u0026amp; 0 \u0026amp; 4,804 \u0026amp; 5,013 \u0026amp; 95.8% \\ \\cmidrule{2-8} \u0026amp; sum \u0026amp; 6,922 \u0026amp; 12,163 \u0026amp; 0 \u0026amp; 5,046 \u0026amp; 24,131 \u0026amp; 97.9%\\textsuperscript{} \\ \u0026amp; $Pr_l$ \u0026amp; 99.6% \u0026amp; 97.9% \u0026amp; 0% \u0026amp; 95.2% \u0026amp; 97.6%\\textsuperscript{*} \u0026amp; 97.7%\\textsuperscript{**} \\ \\bottomrule \\end{tabular} \\caption{Confusion matrix of validation set for tagger $T_{pos}$. *) is class weighted recall/precision-score, **) is $K_1^{tag}$-score} \\label{tabel:RB_per_3} \\end{table}\n\\begin{example} \\label{ex:olijfolie1} The five tokens in description $d$ olijfolie om in te bakken' (in English olive oil for frying purposes\u0026rsquo;) are respectively tagged as quantity', unit\u0026rsquo;, ingredient', ingredient\u0026rsquo;, ingredient' by $T_{sti}$ and as ingredient\u0026rsquo;, unit', ingredient\u0026rsquo;, ingredient', ingredient\u0026rsquo; by $T_{num}$. All tokens are tagged as ingredient' by $T_{pos}$. Since the ground truth labels of all tokens are ingredient\u0026rsquo;, $T_{sti}$ tags $3/5$ correct, $T_{num}$ tags $4/5$ correct and $T_{pos}$ tags $5/5$ correct.\n\\end{example}\n\\subsection{Classification} \\label{subsec:TESTcycle} The classification accuracy $a_C$ for $T_{pos}$ per set of food classes $L_C$ and classification threshold $P_t$ is shown in Figure \\ref{fig:class_per}. Our first main observation is the increasing trend in classification accuracy $a_C$ for each set of food classes $L_C$ from classification threshold $P_t$ of $10%$ to $25%$ (as illustrated by Example \\ref{ex:staafmixer}) and the decreasing trend in classification accuracy $a_C$ for each set of food classes $L_C$ from classification threshold $P_t$ of $30%$ to $70%$ (as illustrated by Example \\ref{ex:bladpeterselie})\n\\begin{figure}[H] %\\includegraphics[width=6cm]{example-image-duck} \\input{Images/pgf/class_acc.pgf} \\caption{Classification accuracy per classification granularity for tagger $T_{pos}$} \\label{fig:class_per} \\end{figure}\n\\begin{example} \\label{ex:staafmixer} The single token in description $d$ staafmixer' (in English hand mixer\u0026rsquo;) is tagged as ingredient' by all taggers, and classified to Hard candies\u0026rsquo; (FoodEx2- code: A034X') for $P_t=10\\%$ and to NFOOD\u0026rsquo; (which is the correct class) for $P_t=25%$. \\end{example}\n\\begin{example} \\label{ex:bladpeterselie} The tokens in description $d$ 0.5 bosje bladpeterselie' (in English 0.5 grove flat-leaf parsley\u0026rsquo;) are respectively tagged as quantity', unit\u0026rsquo; and ingredient' by all taggers. The ingredient-token is classified as Parsley\u0026rsquo; (FoodEx2- code: A00YE') for $P_t=30\\%$ (which is the correct class) and to NFOOD\u0026rsquo; for $P_t=70%$. \\end{example}\nOur second observation in Figure \\ref{fig:class_per} is that the classification accuracy $a_C$ over all classification threshold $P_t$ values is lowest for the set of food classes $L_C$ with highest granularity and highest for the set of food classes $L_C$ with lowest granularity, as illustrated by Example \\ref{ex:biefstuk}.\n\\begin{example} \\label{ex:biefstuk} The tokens in description $d$ 400 g biefstuk' (in English 400 g beefsteak\u0026rsquo;) are respectively tagged as quantity', unit\u0026rsquo; and ingredient' by all taggers. The ingredient-token is classified as Marinated meat\u0026rsquo; (FoodEx2- code: A0EYQ') on a high granularity level (indicated with $E$), where the correct class is Cow, ox or bull fresh meat\u0026rsquo; (FoodEx2- code: A01QX'). The same ingredient-token is classified as Meat and meat products\u0026rsquo; (FoodEx2- code: `A01QR\u0026rsquo;) on a low granularity level (indicated with $H$), which is correct. \\end{example}\nOur third observation in Figure \\ref{fig:class_per} is that the classification accuracy $a_C$ for the set of food classes $L_C$ with granularity $C$ is higher than for the set of food classes $L_C$ with granularity $P$, which is noteworthy since $C$ represents a higher granularity level than $P$.\nThe classification accuracy $a_C$ for classification threshold $P_t=25%$ per set of food classes $L_C$ against ingredient recall $Re_I$ is shown in Figure \\ref{fig:class_error}. Our main observation is that the classification accuracy $a_C$ does not change when ingredient recall $Re_I$ increases from approx. $93.0%$ to approx. $95.5%$, but does increase when ingredient recall increases from approx. $95.5%$ to approx. $99.5%$, as illustrated by Example \\ref{ex:olijfolie}.\n\\begin{figure}[H] %\\includegraphics[width=6cm]{example-image-duck} \\input{Images/pgf/class_acc_breakdown.pgf} \\caption{Classification accuracy $a_C$ for classification threshold $P_t=25%$ per set of food classes $L_C$ against ingredient recall $Re_I$} \\label{fig:class_error} \\end{figure}\n\\begin{example} \\label{ex:olijfolie} (continuation of Example \\ref{ex:olijfolie1}) The ingredient recall $Re_I$ in ingredient description olijfolie om in te bakken' (in English olive oil for frying purposes\u0026rsquo;) increases from $3/5$ for $T_{sti}$, to $4/5$ for $T_{num}$ and to $5/5$ for $T_{pos}$. The predicted food classes are Snacks other than chips and similar' (FoodEx2- code: A06HL\u0026rsquo;) for $T_{sti}$, Olives for oil production' (FoodEx2- code: A016M\u0026rsquo;) for $T_{num}$ and Olive oils' (FoodEx2- code: A036P\u0026rsquo;) for $T_{pos}$. Only the latter food class is correct and contributes to a higher classification accuracy $a_C$. \\end{example}\n\\subsection{Conversion} \\label{subsec:conversion}\nThe mass ratio $r_M$ for $T_{pos}$ per set of food classes $L_C$ and classification threshold $P_t$ is shown in Figure \\ref{fig:conv_per}. Our first observation is the trend of a decreasing mass ratio $r_M$ from classification threshold $P_t$ of $10%$ to $45%$ for all sets of food classes $L_C$, as illustrated by Example \\ref{ex:tomaten}.\n\\begin{figure}[H] %\\includegraphics[width=6cm]{example-image-duck} \\input{Images/pgf/mass_ratio.pgf} \\caption{Mass ratio $r_M$ per set of food classes $L_C$ and classification threshold $P_t$ for $T_{pos}$} \\label{fig:conv_per} \\end{figure}\n\\begin{example} \\label{ex:tomaten} The tokens in ingredient description 2 tomaten' (in English 2 tomatoes\u0026rsquo;) are respectively tagged as quantity' and ingredient\u0026rsquo; by all taggers. The mass is predicted at $370$ grams for $P_t=10%$, based on the assumption that the unit is \u0026lsquo;items\u0026rsquo; (since it is not explicitly states in the ingredient description) and the predicted ingredient class of Tomatoes' (FoodEx2- code: A0DMX\u0026rsquo;). In this case the mass can be predicted since the mass of one tomato item can be looked-up. The mass cannot be predicted for $P_t=20%$ since the predicted food class is `NFOOD\u0026rsquo;, which prevents a look-up of the mass of an item. \\end{example}\nOur second observation in Figure \\ref{fig:conv_per} is that the gap in mass ratio $r_M$ between the set of food classes $L_C$ with the lowest granularity (indicated as $H$) and the other sets, as illustrated by Example \\ref{ex:courgette}.\n\\begin{example} \\label{ex:courgette} The tokens in ingredient description 1 courgette' (in English 1 courgette\u0026rsquo;) are respectively tagged as quantity' and ingredient\u0026rsquo; by all taggers. The mass is predicted at $93$ grams for the set of food classes $L_C$ with the highest granularity (indicated as $E$). As in Example \\ref{ex:tomaten}, the mass is based on the assumption that the unit is \u0026lsquo;items\u0026rsquo; and the predicted ingredient class of Courgettes' (FoodEx2- code: A00JR\u0026rsquo;). For the set of food classes $L_C$ with the lowest granularity (indicated as $H$), the predicted ingredient class of Cucurbits fruiting vegetables' (FoodEx2- code: A00JK\u0026rsquo;). The mass of one item cannot be looked-up for such a broad category. \\end{example}\nOur third observation in Figure \\ref{fig:conv_per} is that the drop in mass ratio $r_M$ from classification threshold $P_t$ of $45%$ to $50%$ for all sets of food classes $L_C$. The drop is explained (see Figure \\ref{fig:conv_error}) by two ingredients that account for about $25%$ of the predicted mass $\\hat{M}$. One is water' (in English water\u0026rsquo;) and the other is ui' (in English onion\u0026rsquo;), both are classified as `NFOOD\u0026rsquo; starting from classification threshold $P_t=50%$.\n\\begin{figure}[H] %\\includegraphics[width=6cm]{example-image-duck} \\input{Images/pgf/mass_ratio_breakdown2.pgf} \\caption{Predicted mass $\\hat{M}$ for tagger $T_{pos}$ and classification granularity $E$} \\label{fig:conv_error} \\end{figure}\n\\subsection{Emission allocation} \\label{subsec:emissionall}\nThe footprint for ten annotated recipes is shown in Figure \\ref{fig:emission_subset}. Our first observation is that the predicted lower bound $\\hat{F}^{5th}$ is not conservative (that is, not lower or equal than the actual lower bound $F^{5th}$) for all recipes, as illustrated by Example \\ref{ex:Karbonades}.\n\\begin{figure}[H] %\\includegraphics[width=6cm]{example-image-duck} \\input{Images/pgf/emission_comparison.pgf} \\caption{Predicted vs annotated footprint with tagger $T_{pos}$, classification granularity $E$ and classification thld. $0.25$} \\label{fig:emission_subset} \\end{figure}\n\\begin{example}\\label{ex:Karbonades} Recipe with ID $5983$ describes the ingredients for the main course Karbonades met geroosterde zoete aardappelsalade' (in English Pork chops with roasted sweet potato salad\u0026rsquo;). Predicted lower bound $\\hat{F}^{5th}$ is higher than the actual lower bound $F^{5th}$ since the ingredient described as 4 karbonades' (in English 4 pork chops\u0026rsquo;). It is classified as Calf fresh meat' (FoodEx2-code: A01QY\u0026rsquo;) with lower bound $E^{5th}=38$ but annotated as Pig fresh meat' (FoodEx2-code: A01RG\u0026rsquo;) with lower bound $E^{5th}=7$.\n\\end{example}\nSecond observation from Figure \\ref{fig:emission_subset} is that the predicted upper bound $\\hat{F}^{95th}$ is not conservative (that is, not higher or equal than the actual upper bound $F^{95th}$) for all recipes, as illustrated by Example \\ref{ex:biefstuk2}.\n\\begin{example}\\label{ex:biefstuk2} Recipe with ID $435$ describes the ingredients for the main course Biefstuk Chimichurri' (in English Beefsteak Chimichurri\u0026rsquo;). Predicted upper bound $\\hat{F}^{95th}$ is lower than the actual upper bound $F^{95th}$ since the ingredient described as 400 g biefstuk' (in English 400 g beefsteak\u0026rsquo;). It is classified as Marinated meat' (FoodEx2-code: A0EYQ\u0026rsquo;) with upper bound $E^{5th}=24$ but annotated as Cow, ox or bull fresh meat' (FoodEx2-code: A01QX\u0026rsquo;) with upper bound $E^{5th}=269$.\n\\end{example}\nDiscussion # In this section we will reflect on our work by describing the limitation of our methodology and the implications of our results.\nLimitations # We discuss the limitations of our methodology in terms of reliability, validity and generalizability. First, we reflect on reliability by considering the size of the annotated data set. The tagging results are reliable since they are tested on over 700 annotated recipes (see Table \\[tabel:splitsizes\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;tabel:splitsizes\u0026rdquo;}). On the other hand, the classification and conversion results are less reliable since they are tested on only 10 annotated recipes, which contain 120 ingredients. The FoodEx2 coding system has approximately 4,000 unique food classes, so more reliable classification and conversion results can be obtained with an annotated set containing multiple ingredients per food class.\nSecondly, we reflect on validity by considering the sources of the input data and assumptions. The GHG emission KB used in this study is not published at the submission date of this work, therefore the validity of the GHG emission values is unknown. We only use the GHG emission values from the KB to illustrate how the pipeline could be used for recipe footprint assessment, without claiming that the emission values are valid. Then, we consider the assumptions and resources used in the conversion task. The mapping, from one food coding system (NEVO) to another food coding system (FoodEx2), that we perform in the conversion task is not based on published work. Consequently, the validity of this mapping is unknown. Also, the assumption that a unit of \u0026lsquo;stuks\u0026rsquo; (in English \u0026rsquo;number of items\u0026rsquo;) can be assumed when the unit is not specified, is not validated. Furthermore, the annotation in terms of FoodEx2 and mass is done by one of the authors. Since this author has no expertise in the FoodEx2 coding system or mass annotation, the validity of ground truth labels is unknown.\nThirdly, we reflect on generalizability. For tagging, the $F_1^{tag}$-scores over the train and validation set are separately measured and found to be consistent (see Appendix \\[app:hyper\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;app:hyper\u0026rdquo;}). As a result, the tagging results are considered generalizable to recipe data sets with similar characteristics as our data set. Nevertheless, generalizability towards data sets from other websites or other languages is not tested. Also, generalizability towards recipe types, that are relatively underrepresented in the train set, is not tested. For classification and conversion, we have no data on which we can base any claims about generalizability of our results. However, since the set of annotated recipes is small with only 120 ingredients compared to 4,000 different classes (see Appendix \\[app:ann\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;app:ann\u0026rdquo;}), the generalizability is assumed to be limited.\nImplications # The implications of our results are structured according to the four tasks defined in our methodology: 1) tagging, 2) classification, 3) conversion and 4) emission allocation. To start with tagging, we argue rule-based tagging is transparent, effective and efficient for the following reasons. It is transparent since the eventual decision tree is human readable and understandable (see Appendix \\[app:trees\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;app:trees\u0026rdquo;}). Rule-based tagging is effective since an $F_1^{tag}$-score of 98% is reached, which is considered high. Rule-based tagging is efficient since only ten input features are used. Furthermore, the results imply that specific features are effective to distinguish tokens with specific tag labels. For instance, the two features description length and token position. Since no quantity tokens are tagged as units or vise versa when these two feature are used (see Table \\[tabel:RB_per_1\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;tabel:RB_per_1\u0026rdquo;}), they are effective to distinguish quantity tokens from unit tokens. Also, the numerical-like feature can effectively distinguish quantity tokens from ingredient tokens, since the addition of this numerical-like feature led to a reduction of the number of ingredient tokens labeled as quantities or vise versa (see Table \\[tabel:RB_per_2\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;tabel:RB_per_2\u0026rdquo;}). In addition, the POS features can effectively distinguish unit tokens from ingredient tokens, since the addition of these POS features led to a reduction of ingredient tokens labeled as unit tokens or vise versa (Table \\[tabel:RB_per_3\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;tabel:RB_per_3\u0026rdquo;}).\nSecondly, the implications regarding classification are discussed based on Hypothesis \\[hy:recall\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;hy:recall\u0026rdquo;}a, \\[hy:granularity\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;hy:granularity\u0026rdquo;}a and \\[hy:threshold\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;hy:threshold\u0026rdquo;}a. Our results do not support Hypothesis \\[hy:recall\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;hy:recall\u0026rdquo;}a, which states a higher ingredient recall $Re_I$ leads to a higher classification accuracy $a_C$. Namely, similar classification accuracy values $a_C$ are observed in pipeline configurations with a different ingredient recall $Re_I$ (see Figure \\[fig:conv_error\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;fig:conv_error\u0026rdquo;}). A potential explanation could be that all ingredient description tokens need to be recalled for correct classification (see Example \\[ex:olijfolie\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;ex:olijfolie\u0026rdquo;}). Also, our results do not support Hypothesis \\[hy:granularity\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;hy:granularity\u0026rdquo;}a, which states that a more granular set of food classes $L_C$ leads to a lower prediction accuracy $a_C$. We did observe scenarios, in which a more granular set of food classes $L_C$ led to a lower prediction accuracy $a_C$. However, we also observed the opposite (see Figure \\[fig:class_per\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;fig:class_per\u0026rdquo;}). A potential explanation for a higher prediction accuracy $a_C$, given a less granular set of food classes, is provided in Example \\[ex:biefstuk\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;ex:biefstuk\u0026rdquo;}. A potential explanation for a lower prediction accuracy $a_C$, given a less granular set of food classes, could be the irregularities in the FoodEx2 hierarchy. Additionally, our results do not support Hypothesis \\[hy:threshold\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;hy:threshold\u0026rdquo;}a, which states that a higher classification threshold $P_t$ leads to a higher classification accuracy $a_C$. We observe two trends, one in which classification accuracy $a_C$ increases for higher classification threshold values up to $P_t=0.25$, and a decreasing trend for a further increasing classification threshold (see Figure \\[fig:class_per\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;fig:class_per\u0026rdquo;}). These two trends could be explained by classifying utensils correctly as non-food or by classifying food incorrectly as non-food (see Examples \\[ex:staafmixer\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;ex:staafmixer\u0026rdquo;} and \\[ex:bladpeterselie\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;ex:bladpeterselie\u0026rdquo;}).\nThirdly, the implications regarding conversion are discussed based on Hypothesis \\[hy:recall\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;hy:recall\u0026rdquo;}b, \\[hy:granularity\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;hy:granularity\u0026rdquo;}b and \\[hy:threshold\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;hy:threshold\u0026rdquo;}b. The results do not support Hypothesis \\[hy:recall\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;hy:recall\u0026rdquo;}b, which states a higher quantity or unit recall leads to higher mass ratio $r_M$. First of all, the quantity recall $Re_Q$ did not change over the different pipeline configurations. Therefore we have no data to base any claims on quantity recall changes. Furthermore, lower mass ratio $r_M$ values are observed for pipeline configurations with a higher unit recall $Re_U$. Our results do not support Hypothesis \\[hy:granularity\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;hy:granularity\u0026rdquo;}b, which states that a more granular set of food classes $L_C$ leads to a higher mass ratio $r_M$. We observed that four sets $L_C$ with different granularity levels (\u0026lsquo;E\u0026rsquo;, \u0026lsquo;C\u0026rsquo;, \u0026lsquo;P\u0026rsquo; and \u0026lsquo;M\u0026rsquo;) score very similar in terms of mass ratio and one set (\u0026lsquo;H\u0026rsquo;) scores 20-30% lower. This could be explained by the conversion from FoodEx2 to NEVO code (see Example \\[ex:courgette\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;ex:courgette\u0026rdquo;}). In addition, our results do not support Hypothesis \\[hy:threshold\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;hy:threshold\u0026rdquo;}b, which states that a higher classification threshold $P_t$ leads to a lower mass ratio $r_M$. This can be explained by the fact that non-food items, that is, kitchen utensils, do not contribute to a recipe\u0026rsquo;s mass. Therefore no gain in mass ratio can be reached by classifying ingredient descriptions as non-food, which is what the threshold $P_t$ does.\nFourth, the implications regarding emission allocation. Our design decision, to assume the most conservative GHG emission values in the KB if the prediction class $\\hat{C}=NFOOD$, is no guarantee for conservative footprint $\\hat{F}$ assessment (see Figure \\[fig:emission_subset\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;fig:emission_subset\u0026rdquo;}, and Examples \\[ex:Karbonades\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;ex:Karbonades\u0026rdquo;} and \\[ex:biefstuk2\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;ex:biefstuk2\u0026rdquo;}). Both examples show that the conservative assumption did not compensate for the error in footprint $\\hat{F}$ caused by incorrect classification of ingredients that account for a relatively high share of the recipe\u0026rsquo;s total mass.\nConclusion # We assessed the footprint of recipes in terms of GHG emission that can be allocated to its ingredients. We developed and empirically studied a pipeline to link Dutch recipes to a GHG emission KB which is structured by a food coding system called FoodEx2. We conclude that, although all sub-tasks are executed successfully to a large or very large extent, automated recipe footprint assessment can be done to a limited extent. Therefore, our pipeline can be considered a proof-of-technology, but not a proof-of-concept. We have four arguments, one per sub question as posed in Section \\[sec:intro\\]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;sec:intro\u0026rdquo;}, to support our conclusion. Firstly, descriptions of ingredients, quantities and units can be recognized to a very large extent given our observed $F_1^{tag}$-scores up to 98%. Secondly, recognized ingredient descriptions can to a large extent be correctly linked to a FoodEx2 coded GHG KB, as our 10-recipe test case resulted in a classification accuracy $a_C$ up to 77%. Thirdly, recognized quantity and unit descriptions can to a very large extent be linked to a GHG KB, as our 10-recipe test case showed that descriptions of quantities and units can be converted to numerical values with a mass ratio $r_M$ up to 97%. Lastly, we argue for a proof-of-technology rather than a proof-of-concept because our pipeline did not always result in conservative footprint predictions $\\hat{F}$.\nSuggestions for future work are as follows. First, a requirement study could be done to specify what the GHG emission precision requirements are for use cases such as footprint trend monitoring or sustainable recipe recommendation. Secondly, a resource study could be done to select a food classification system and corresponding GHG emission KB that enables footprint assessment that meets the requirements specified in the requirement study mentioned earlier. Ideally, a distinction could be made between a classification system and KB that would be sufficiently precise assuming a) perfect prediction and b) conservative prediction. The latter could be used for our third suggestion, that is, to design and/or fine-tune a pipeline that can predict conservative GHG emission values that meet the requirements of the resource study mentioned earlier. A public recipe data set with golden standard would be a required asset to enable comparison between different pipeline designs or configurations.\nAcknowledgements # Throughout the writing of this thesis, I have received a great deal of support. I would like to express my gratitude to my supervisors Prof. Dr. Paul Groth (University of Amsterdam) and Dr. Marieke van Erp (KNAW Humanities Cluster DHLab / KNAW HuC DHLab) for their guidance and feedback. In addition, I would like to thank Christian Reynolds (Centre for Food Policy City, University of London), Marja Beukers (Rijksinstituut voor Volksgezondheid en Milieu), and Carsten Behring (European Food Safety Authority) for providing the required resources for this thesis.\nList of symbols # Problem formulation and model # $R$ recipe object $d$ description of ingredient or utensil, sequence of tokens $D$ set of descriptions $d$ $d^{tag}$ description of ingredient or utensil, sequence of labeled tokens $\\hat{d}^{tag}$ prediction for sequence of labeled tokens $d^{tag}$ $D^{tag}$ set of sequences of labeled tokens $d^{tag}$ $\\hat{D}^{tag}$ set of predictions for sequences of labeled tokens $d^{tag}$ $U$ token label which stands for \u0026lsquo;unit measure\u0026rsquo; $Q$ token label which stands for \u0026lsquo;quantity\u0026rsquo; $I$ token label which stands for \u0026lsquo;ingredient\u0026rsquo; $F$ recipe footprint defined as 5^th^ percentile $F^{5th}$, mean $F^{mean}$ and 95^th^ percentile $F^{95th}$ of the distribution of Green House Gas emission per unit mass allocated to a dish \\[kgCO2eq/kg\\] $\\hat{F}$ prediction for recipe footprint $F$ $I_q$ quantified ingredient object $C$ food class $\\hat{C}$ prediction for food class $M$ food mass $\\hat{M}$ prediction for food mass $E$ 5^th^ percentile $E^{5th}$, mean $E^{mean}$ and 95^th^ percentile $E^{95th}$ of the distribution of Green House Gas emission per unit mass allocated to an ingredient \\[kgCO2eq/kg\\] $\\hat{E}$ prediction for ingredient Green House Gas emission per unit mass $E$\nExperimental design # $T$ tagging model $T \\in {T_{sti}, T_{num}, T_{pos}}$ $T_{sti}$ tagging model with description token count and position as features $T_{num}$ tagging model with features of $T_{sti}$ plus feature that describes if token is numerical $T_{pos}$ tagging model with features of $T_{num}$ plus feature that describes Part-Of-Speech tag $L_C$ food classification set $C \\in L_C$ $P_t$ food classification threshold $Pr_l$ precision for token label $l \\in {Q,U,I}$ $Rr_l$ recall for token label $l \\in {Q,U,I}$ $Pr$ macro averaged precision $Rr$ macro averaged recall $F_1^{tag}$ F-1 score for tagging of tokens $a_C$ classification accuracy $r_M$ mass ratio\nRecipe data # Our exploratory data analysis is summarized as follows. The publication date of the recipes ranges from 2013 up to 2021, more than half of the recipes were published in 2013. The recipes are categorized, the five largest categories are Main ($49%$), Unknown ($18%$), Dessert ($11%$), Appetizer ($8%$) and Lunch ($6%$). Figure 1{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;fig:eda2\u0026rdquo;} shows a segmentation of all tokens in the data set by description length (i.e. the length of the description expressed in number of tokens), token position (first token in the description has position 0, the second token has position 1, etc.) and dominant token label (ingredient, unit or quantity). The percentage shown in each segment describes the share of tokens that is labeled with the dominant label.\nSegmentation of tokens by description length, token position and dominant label. The percentage shown in each segment describes the share of tokens that is labeled with the dominant label Emission knowledge base # Figure 2{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;fig:ghge1\u0026rdquo;} show an example of GHG emission for food class \u0026lsquo;Cherry tomatoes\u0026rsquo; and all parent nodes in the FoodEx2 hierarchy.\nExample of GHG emission for food class Cherry tomatoes and all parent nodes in the FoodEx2 hierarchy Tagger design # Figure 3{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;fig:RB_hyper_1\u0026rdquo;} shows the $F_1^{tag}$-scores for a grid search over all combinations of hyperparameters maximum tree depth and initial state for $T_{sti}$.\nHyperparameter tuning for tagger T1 Our two main interpretations of the data shown in Figure 3{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;fig:RB_hyper_1\u0026rdquo;} are that 1) the model is not over-fitted on the training set since $F_1^{tag}$-scores over the train and validation set are close to identical and 2) given the model type and feature set, the $F_1^{tag}$-score seems to be bounded by a maximum value at approximately 94% since $F_1^{tag}$-score converges towards this bound when the maximum tree depth goes to infinity. We choose to set the hyperparameters of $T_{sti}$ to maximum tree depth of $3$ and initial state of $4$ since this is given the lowest model complexity at which the accuracy is (close to) the maximum bound.\nFigure 4{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;fig:RB_hyper_2\u0026rdquo;} shows the $F_1^{tag}$-scores for a grid search over all combinations of hyperparameters maximum tree depth and initial state for $T_{num}$. Our main interpretation of the data shown in Figure 4{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;fig:RB_hyper_2\u0026rdquo;} is that the additional LIKE-NUM features lead to an increase in the maximum bound on the $F_1^{tag}$-score since the a maximum value is approximately 96% (increase of 2% compared to $T_{sti}$).\nHyperparameter tuning for tagger Tnum Figure 5{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;fig:RB_error_1\u0026rdquo;} shows a breakdown of the incorrectly labelled tokens by $T_{num}$ in the train set by Part-Of-Speech (POS). Three additional binary features are selected for $T_{pos}$: the features describe if a token has \u0026lsquo;ADJ\u0026rsquo;, \u0026lsquo;ADP\u0026rsquo; or \u0026lsquo;CCONJ\u0026rsquo; POS tag.\nError analysis of tagger T2 The results of hyperparameter tuning based on these additional features is shown in Figure 4{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;fig:RB_hyper_2\u0026rdquo;}. Our main interpretation of the data shown in Figure 6{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;fig:RB_hyper_3\u0026rdquo;} is that the additional POS features lead to an increase in the maximum bound on the accuracy score since the a maximum value is approximately 98% (increase of 2% compared to $T_{num}$).\nHyperparameter tuning for tagger Tpos Decision trees # Decision tree for Tagger Tsti # |-- token_index_1 \u0026lt;= 0.4 | |-- token_index_0 \u0026lt;= 0.3 | | |-- token_index_2 \u0026lt;= 0.6 | | | |-- class: 2 | | |-- token_index_2 \u0026gt; 0.6 | | | |-- class: 2 | |-- token_index_0 \u0026gt; 0.3 | | |-- n_tokens_1 \u0026lt;= 2.6 | | | |-- class: 2 | | |-- n_tokens_1 \u0026gt; 2.6 | | | |-- class: 0 |-- token_index_1 \u0026gt; 0.4 | |-- n_tokens_3 \u0026lt;= -0.0 | | |-- n_tokens_2 \u0026lt;= 1.1 | | | |-- class: 2 | | |-- n_tokens_2 \u0026gt; 1.1 | | | |-- class: 0 | |-- n_tokens_3 \u0026gt; -0.0 | | |-- class: 0 Decision tree for Tagger Tnum # |-- token_index_0 \u0026lt;= 0.3 | |-- token_index_1 \u0026lt;= 0.4 | | |-- token_index_2 \u0026lt;= 0.6 | | | |-- class: 2 | | |-- token_index_2 \u0026gt; 0.6 | | | |-- class: 2 | |-- token_index_1 \u0026gt; 0.4 | | |-- n_tokens_2 \u0026lt;= 1.1 | | | |-- class: 2 | | |-- n_tokens_2 \u0026gt; 1.1 | | | |-- class: 0 |-- token_index_0 \u0026gt; 0.3 | |-- n_tokens_2 \u0026lt;= 1.1 | | |-- NUM \u0026lt;= 0.5 | | | |-- class: 0 | | |-- NUM \u0026gt; 0.5 | | | |-- class: 2 | |-- n_tokens_2 \u0026gt; 1.1 | | |-- NUM \u0026lt;= 0.5 | | | |-- class: 0 | | |-- NUM \u0026gt; 0.5 | | | |-- class: 1 Decision tree for Tagger Tpos # |-- NUM \u0026lt;= 0.5 | |-- ADJ \u0026lt;= 1.4 | | |-- n_tokens_2 \u0026lt;= 1.1 | | | |-- token_index_1 \u0026lt;= 0.4 | | | | |-- token_index_2 \u0026lt;= 0.6 | | | | | |-- n_tokens_3 \u0026lt;= -0.0 | | | | | | |-- class: 0 | | | | | |-- n_tokens_3 \u0026gt; -0.0 | | | | | | |-- class: 0 | | | | |-- token_index_2 \u0026gt; 0.6 | | | | | |-- ADP \u0026lt;= 2.3 | | | | | | |-- class: 0 | | | | | |-- ADP \u0026gt; 2.3 | | | | | | |-- class: 0 | | | |-- token_index_1 \u0026gt; 0.4 | | | | |-- ADP \u0026lt;= 2.3 | | | | | |-- CCONJ \u0026lt;= 6.0 | | | | | | |-- class: 0 | | | | | |-- CCONJ \u0026gt; 6.0 | | | | | | |-- class: 0 | | | | |-- ADP \u0026gt; 2.3 | | | | | |-- n_tokens_3 \u0026lt;= -0.0 | | | | | | |-- class: 0 | | | | | |-- n_tokens_3 \u0026gt; -0.0 | | | | | | |-- class: 0 | | |-- n_tokens_2 \u0026gt; 1.1 | | | |-- class: 0 | |-- ADJ \u0026gt; 1.4 | | |-- token_index_2 \u0026lt;= 0.6 | | | |-- token_index_1 \u0026lt;= 0.4 | | | | |-- class: 0 | | | |-- token_index_1 \u0026gt; 0.4 | | | | |-- n_tokens_2 \u0026lt;= 1.1 | | | | | |-- n_tokens_3 \u0026lt;= -0.0 | | | | | | |-- class: 0 | | | | | |-- n_tokens_3 \u0026gt; -0.0 | | | | | | |-- class: 0 | | | | |-- n_tokens_2 \u0026gt; 1.1 | | | | | |-- class: 0 | | |-- token_index_2 \u0026gt; 0.6 | | | |-- n_tokens_3 \u0026lt;= -0.0 | | | | |-- class: 0 | | | |-- n_tokens_3 \u0026gt; -0.0 | | | | |-- class: 0 |-- NUM \u0026gt; 0.5 | |-- n_tokens_2 \u0026lt;= 1.1 | | |-- n_tokens_3 \u0026lt;= -0.0 | | | |-- token_index_0 \u0026lt;= 0.3 | | | | |-- token_index_2 \u0026lt;= 0.6 | | | | | |-- token_index_1 \u0026lt;= 0.4 | | | | | | |-- class: 2 | | | | | |-- token_index_1 \u0026gt; 0.4 | | | | | | |-- class: 2 | | | | |-- token_index_2 \u0026gt; 0.6 | | | | | |-- class: 2 | | | |-- token_index_0 \u0026gt; 0.3 | | | | |-- class: 2 | | |-- n_tokens_3 \u0026gt; -0.0 | | | |-- token_index_1 \u0026lt;= 0.4 | | | | |-- class: 2 | | | |-- token_index_1 \u0026gt; 0.4 | | | | |-- class: 0 | |-- n_tokens_2 \u0026gt; 1.1 | | |-- class: 1 Annotation # This Section describes the annotated subset of recipes, the rationale for this selection, before showing the results in terms of linking. The selection is made with the objective to be as representative as possible for the main dishes in the test set. So ideally we would sample ingredients from the main dish recipes in such a way that the distribution over the predicted classification codes is even. The main complication is that we have resources to annotate 10 recipes which topically contain about 140 ingredients in total, but the classification model can give about 4,000 different FoodEx2-codes as output. To be as representative as possible we selected a subset of recipes of which the ingredients have a modal value of unique predicted classification codes. Figure 7{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;fig:hist_unique\u0026rdquo;} shows the distribution of number of unique prediction classes $\\hat{C}$ over 1,000 random subsets of 10 main course recipes, and highlights the bin from which we randomly selected the subset for annotation.\nDistribution of number of unique prediction classes over 1,000 subsets of 10 main course recipes https://www.allrecipes.com\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.nltk.org\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://stanfordnlp.github.io/CoreNLP/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://doi.org/10.17026/dans-xvh-x9wz\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.english-corpora.org/hansard\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/robzwitserlood/NL-recipe-sustainability\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"1 January 2021","externalUrl":null,"permalink":"/post/2021-01-01-thesis/","section":"Posts","summary":"","title":"About me","type":"post"},{"content":"","date":"1 January 2021","externalUrl":null,"permalink":"/post/","section":"Posts","summary":"","title":"Posts","type":"post"},{"content":" Rob Zwitserlood # Residence: Amsterdam, The Netherlands\nContact: E-mail:\nLinks:\nLanguages: Dutch (mother tongue), English (fluent)\nAbout Me # \u0026ldquo;Those who strive for simplicity, will first have to master the complexity.\u0026rdquo;\nI\u0026rsquo;m an analytical, inquisitive, and independent young professional with 3+ years of work experience. After a career change to Data Science, I\u0026rsquo;m now looking for a research-oriented job opportunity in the public sector with a positive impact on society and the environment.\nWork Experience # Product (Marketing) Manager # TomTom\n04/2019  07/2020\nManaged the \u0026lsquo;connected services\u0026rsquo; in the B2B automotive portfolio. Acted as internal product expert and drove product development based on market insights. Skills: Market analysis, pricing, project management, stakeholder management. Strategy Consultant # Roland Berger\n05/2016  09/2018\nOwned a module of a strategic advice for board-level executives. Worked on projects in diverse industries: Oil \u0026amp; Gas, sustainable energy, and food. Skills: Structure complex problems, client communication, business modelling. Intern Strategy Consultant # Roland Berger\n01/2016  04/2016\nSupported a module owner of a strategic advice for board-level executives. Projects in construction and food industry. Skills: Data-driven analysis, storylining, presentation, interviewing. Various Secondary Jobs # 04/2009  01/2015\nStudent assistant, Eindhoven University of Technology. Assistant teacher Math, SSL Leiden. Languages # Dutch (mother tongue) English (fluent) ","externalUrl":null,"permalink":"/cv/","section":"","summary":"","title":"","type":"page"},{"content":"My name is Rob, en paul is er ook\nmy history # To be honest, I\u0026rsquo;m having some trouble remembering right now\n","externalUrl":null,"permalink":"/about/","section":"","summary":"","title":"About me","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]